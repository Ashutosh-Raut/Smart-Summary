<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="style.css">
    <!-- down chevron -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- right chevron -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0">
    <!-- sell tag -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- location  -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- email -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- search -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- translate -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0">

    <!-- calendar -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- play -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- note -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- air -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- plus -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- 3 -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">
    <!-- code -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200">

    <title>Smart Summary</title>
</head>

<body>

    <nav class="navbar">

        <div class="top-container">

            <ul class="dropdowns">

                <li class="dropdown">
                    <a href="#" class="dropdown-text">University<span class="material-symbols-outlined">expand_more</span></a>
                </li>

                <li class="dropdown">
                    <a href="#" class="dropdown-text">Branch<span class="material-symbols-outlined">expand_more</span></a>
                </li>

            </ul>

            <div id="logo"><a href="main.html"><img src="logo.png" height="30px" width="160px"></a></div>

            <ul class="interactions">
                
                <li class="sign-in"><a href="#" style="padding: 0 20px 0 20px;">Sign In</a></li>
            </ul>
        </div>
    </nav>
    <div class="topic-container">
        <ul class="topics">
            <li><a href="dsa.html"><div class="footer-link-heading">DSA</div></a></li>
            <li><a href="ai.html">Artificial Intelligence</a></li>
            <li><a href="ml.html">Machine Learning</a></li>
            <li><a href="cc.html">Cloud Computing</a></li>
            <li><a href="se.html">Software Engineering</a></li>
            <li><a href="cn.html">Computer Network</a></li>
            <li><a href="os.html">Operating System</a></li>
            <li><a href="dbs.html">DBMS</a></li>
            <li><a href="oops.html">OOPs</a></li>
            <li><a href="daa.html">DAA</a></li>
        </ul>
    </div>

    <main>

        <div class="article-container">

            <div class="sidebar">
                <ul class="sidebar-menu">
                    <li><a href="#unit1">UNIT 1: Introduction</a></li>
                    <li><a href="#unit2">UNIT 2: Stacks and Queues</a></li>
                    <li><a href="#unit3">UNIT 3: Linked list</a></li>
                    <li><a href="#unit4">UNIT 4: Trees and Graphs</a></li>
                    <li><a href="#unit5">UNIT 5: Searching and Sorting</a></li>
                </ul>
            </div>

            <div class="main-content">

                <div id="unit1" class="formatted-text">
                    <!-- Unit 1 text -->
                    <h2>UNIT 1: Introduction</h2>
                    <br>
                    <section>
                        <h2>Data</h2><br>
                        <p>Data is the raw information processed and utilized by computer systems. It comes in various types like integers, strings, etc., and can be represented in different formats such as binary or decimal. Structuring data efficiently is vital, achieved through various data structures like arrays, trees, and graphs, each offering distinct advantages. Operations like reading, writing, and sorting can be performed on data, and its size can range from individual variables to large datasets.</p><br>
                        <p>Understanding data is foundational in computing, aiding in algorithm design and problem-solving. Efficient data management enhances system performance and resource utilization, crucial in database organization and operating system design. Data integrity and security are paramount, safeguarding against unauthorized access and ensuring reliability.</p><br>
                        <p>Data analysis yields insights crucial for informed decision-making, benefiting areas like business intelligence and healthcare. Linear data structures arrange elements sequentially, while nonlinear structures like trees and graphs offer alternative organization methods. They find applications in databases, operating systems, computer graphics, and artificial intelligence.</p><br>
                        <p>Data structures offer advantages such as efficiency, flexibility, reusability, and maintainability. They allow for optimized storage, retrieval, and manipulation of data, improving performance in critical applications. Well-designed data structures simplify program understanding, modification, and maintenance, enhancing overall software quality and longevity. Understanding these concepts is essential in engineering exams for designing effective algorithms and software systems.</p><br>
                    </section>
                    <section>
                        <h2>Data types</h2><br>
                        <p>Data structures are crucial elements in computer science and engineering, organizing and storing data efficiently for processing. They encompass various data types, including integers, floating-point numbers, characters, and booleans, each with unique characteristics and applications. Basic data types like arrays, structures, and pointers facilitate complex data organization and manipulation.</p><br>
                        <p>Linear data structures arrange elements sequentially, while non-linear structures like trees and graphs offer alternative organization methods. Arrays, for instance, support operations like searching, sorting, and insertion efficiently.</p><br>
                        <p>Linked lists, on the other hand, enable dynamic memory allocation and flexible insertion and deletion operations. Stacks are particularly useful for implementing recursion, while queues are valuable in managing resource allocation and scheduling tasks.</p><br>
                        <p>Trees, including binary trees and search trees, facilitate hierarchical data representation and efficient searching. Graphs, consisting of vertices and edges, find applications in modeling complex relationships, such as social networks or routing problems.</p><br>
                        <p>Advantages of data structures include improved organization, faster data retrieval, and algorithm design. However, they come with disadvantages like increased computational overhead and complexity in design and debugging.</p><br>
                        <p>Understanding the need and characteristics of different data structures is crucial for choosing the right one for a given problem, ensuring optimal performance and efficiency in engineering applications.</p><br>
                        <p>Reference:<br>Various textbooks like "Introduction to Algorithms" by Cormen et al. and online platforms such as Coursera and Udemy offer comprehensive courses on data structures and algorithms, providing valuable resources for further study and understanding.</p><br>
                    </section>

                    <section>
                        <h2>Data Structures</h2>
                        <br>
                        <p>Data structures are methods of organizing and storing data efficiently in a computer. They define the relationships between data values and the operations that can be performed on them.</p>
                        <h3>Key Concepts</h3>
                        <ol>
                            <li><strong>Organization:</strong> Provides systematic ways to organize and manage data for efficient storage, retrieval, and manipulation.</li><br>
                            <li><strong>Access Methods:</strong> Defines methods for accessing and retrieving data elements, including insertion, deletion, traversal, search, and sorting.</li><br>
                            <li><strong>Efficiency:</strong> Aims to optimize the efficiency of data access and manipulation operations in terms of time and space complexity.</li><br>
                            <li><strong>Abstraction:</strong> Offers a level of abstraction that hides implementation details, promoting modularity, reusability, and maintainability.</li><br>
                            <li><strong>Applications:</strong> Widely used across domains like databases, operating systems, compilers, networking, artificial intelligence, etc., as building blocks for designing efficient algorithms and solving complex problems.</li><br>
                        </ol>
                        <h3>Examples</h3>
                        <ul>
                            <li><strong>Arrays:</strong> Store elements of the same type in contiguous memory locations, offering efficient random access.</li><br>
                            <li><strong>Linked Lists:</strong> Linear data structures consisting of nodes linked by pointers, supporting dynamic memory allocation and efficient insertion/deletion.</li><br>
                            <li><strong>Trees:</strong> Hierarchical data structures composed of nodes connected by edges, commonly used for representing hierarchical relationships.</li><br>
                            <li><strong>Graphs:</strong> Non-linear data structures consisting of vertices and edges, widely used for modeling relationships between objects.</li><br>
                        </ul>
                        <h3>Types</h3>
                        <ul>
                            <li><strong>Linear Data Structures:</strong> Elements are arranged in a single dimension (e.g., lists, stack, queue).</li><br>
                            <li><strong>Non-Linear Data Structures:</strong> Elements are arranged in multiple dimensions (e.g., tree, graph).</li><br>
                        </ul>
                        <h3>Applications</h3>
                        <p>Used in various fields such as operating systems, graphics, computer design, blockchain, genetics, image processing, and simulation.</p><br>
                    </section>
                    <section>
                        <h2>Abstract Data Type (ADT)</h2>
                        <br>
                        <p>Abstract Data Types (ADTs) serve as mathematical models defining a set of data values and operations on those values, abstracting away storage and implementation details. They encapsulate data and operations, promoting modularity, reusability, and abstraction.</p>
                        <h3>Key Concepts</h3>
                        <ol>
                            <li><strong>Encapsulation:</strong> Hides internal details, ensuring data integrity and providing a public interface for interaction.</li><br>
                            <li><strong>Abstraction:</strong> Enhances conceptualization of real-world scenarios, promotes robust programming, and facilitates error detection.</li><br>
                            <li><strong>Modularity:</strong> Offers independence in data structure choice, accommodating different implementations without altering functionality.</li><br>
                            <li><strong>Information Hiding:</strong> Prevents unauthorized access and misuse, supporting secure data manipulation.</li><br>
                        </ol>
                        <h3>Advantages</h3>
                        <p>ADTs abstract data manipulation, providing a black-box view where users need not concern themselves with internal implementations. They enhance conceptualization of real-world scenarios, promote robust programming, and facilitate error detection.</p><br>
                        <h3>Challenges</h3>
                        <p>Despite their advantages, ADTs entail overhead in terms of memory and processing, potentially impacting performance. Their implementation complexity and learning curve require investment in understanding and development.</p><br>
                        <h3>Applications</h3>
                        <p>Widely used in software development to efficiently manage and manipulate data, offering structured and flexible solutions. ADTs provide a powerful toolset for engineers, enabling them to build scalable and maintainable software systems adaptable to diverse requirements and scenarios.</p><br>
                    </section>

                    <section>
                        <h2>Data Representation</h2>
                        <p>Data representation in computer science involves various techniques for storing and organizing information efficiently. It encompasses encoding methods, storage techniques, compression strategies, and serialization formats to optimize access, manipulation, and resource utilization. Key aspects include data encoding, storage, compression, serialization, and representation in algorithms.</p><br>
                        <h3>Binary System</h3>
                        <p>The binary system serves as the foundation of data representation, with 0s and 1s denoting 'off' and 'on' states respectively. Bits and bytes are fundamental units, with a byte typically consisting of 8 bits. Number systems like decimal and hexadecimal, along with character encodings such as ASCII and Unicode, are crucial for representing numbers and text in computer systems.</p><br>
                        <h3>Data Encoding</h3>
                        <p>Data representation involves expressing data in binary format, while interpretation refers to computers' ability to understand and process this encoded data. Binary digits represent electric current states, with bits grouped into bytes, kilobytes, megabytes, and beyond for efficient organization and management.</p><br>
                        <h3>Number Systems</h3>
                        <p>Decimal and hexadecimal systems complement the binary system, aiding in processing numeric and textual data. Character encodings like ASCII and Unicode translate human-readable characters into binary code. Various encoding schemes interpret data in different formats, such as images using JPG, PNG, and GIF, and audio files using MP3 and WAV formats.</p><br>
                        <h3>Importance</h3>
                        <p>Understanding data representation and interpretation is essential in computer science, enabling developers to make informed decisions about storage, manipulation, and transmission of data. By leveraging encoding schemes, storage techniques, and serialization formats, developers can design efficient algorithms and software systems while optimizing performance and resource usage.</p><br>
                    </section>

                    <section>
                        <h2>Characteristics</h2><br>
                        <h3>Characteristics of Algorithms</h3><br>
                        <p>Algorithms are fundamental in computer science, providing step-by-step instructions for solving problems efficiently. Key aspects include correctness, efficiency, optimality, determinism, scalability, robustness, and adaptability. Correctness ensures the algorithm produces the desired output for all inputs, while efficiency refers to executing tasks within reasonable time and resource constraints. Optimality aims to achieve tasks using minimal resources, while determinism ensures consistent outputs for the same inputs.</p><br>
                        <p>Scalability is essential for handling increasing data or computational loads without significant performance decrease. Robust algorithms gracefully handle unexpected or erroneous input, while adaptive algorithms adjust behavior based on changing conditions. Algorithms find applications in various fields like computer science, mathematics, operations research, artificial intelligence, and data science.</p><br>
                        <h4>Algorithm Design</h4><br>
                        <p>Characteristics of algorithms include clarity, well-defined inputs and outputs, finiteness, feasibility, and language independence. Algorithms can be categorized into types like brute force, recursive, backtracking, searching, sorting, hashing, divide and conquer, greedy, dynamic programming, and randomized algorithms. They offer advantages such as simplicity and step-wise problem-solving, but drawbacks include time-consuming writing and complex logic comprehension.</p><br>
                        <p>Algorithm design involves defining the problem, considering constraints, specifying inputs and outputs, and creating a solution within given constraints. Algorithms can be expressed through natural language, flowcharts, or pseudo code. Analysis of algorithms involves priori analysis before implementation and posterior analysis after implementation to assess correctness, space, and time complexities.</p><br>
                        <p>Time and space complexities measure an algorithm's efficiency in terms of time and memory requirements. Space complexity depends on the number of variables and data types, while time complexity accounts for constant and variable time components. Expressing algorithms in pseudo code is preferred for its simplicity and clarity, making it understandable even to those with limited programming knowledge.</p><br>
                    </section>
                    <section>
                        <h2>Program Characteristics</h2>
                        <br>
                        <p>In the realm of data structures, programs are vital components that manipulate the stored data through sequences of instructions or code segments. These programs enable software systems to execute tasks like data processing, information retrieval, and algorithmic computations effectively. Key aspects of programs in data structures include functionality, modularity, abstraction, encapsulation, reusability, efficiency, robustness, and scalability.</p><br>
                        <h3>Functionality</h3><br>
                        <p>Programs define the behavior of software systems by specifying operations on data structures and the algorithms used to manipulate them. They enforce rules governing data access, modification, and processing.</p><br>
                        <h4>Modularity</h4><br>
                        <p>Programs are divided into smaller, self-contained modules, making them easier to understand, maintain, and extend. Changes to one module do not necessarily impact the entire system.</p><br>
                        <h4>Abstraction</h4><br>
                        <p>Abstraction hides unnecessary details, exposing only essential features or interfaces of a program. This simplifies working with complex data structures and algorithms.</p><br>
                        <h4>Encapsulation</h4><br>
                        <p>Encapsulation combines data and methods into a single unit, allowing for information hiding and controlled access to data.</p><br>
                        <h4>Reusability</h4><br>
                        <p>Well-designed programs are modular and reusable, allowing developers to leverage existing code components in new contexts or applications without modification, saving time and effort.</p><br>
                        <h4>Efficiency</h4><br>
                        <p>Efficient programs execute tasks timely and minimize resource consumption, optimizing computational resources to achieve desired performance levels.</p><br>
                        <h4>Robustness</h4><br>
                        <p>Robust programs handle unexpected or erroneous inputs gracefully, incorporating error handling mechanisms and defensive programming techniques to prevent failures and ensure reliability.</p><br>
                        <h4>Scalability</h4><br>    
                        <p>Scalable programs accommodate growth in data volume, user traffic, or system complexity without significant performance degradation, adapting to changing requirements and workload demands efficiently.</p><br>
                        <p>Programs in data structures drive the functionality and behavior of software systems. By effectively utilizing data structures and algorithms, developers can create robust, efficient, and scalable solutions for various computational problems and application domains.</p><br>
                    </section>
                    
                    <section>
                        <h3>Analyzing programs:</h3>
                        <p>Analyzing programs in the context of data structures is a critical step in software development, aiming to evaluate their behavior, performance, and efficiency. This process involves several key aspects:</p><br>
                        <ol>
                            <li>Performance Profiling: It entails measuring runtime behavior, identifying hotspots, and detecting resource-intensive operations using tools like profilers.</li><br>
                            <li>Time Complexity Analysis: Understanding computational time relative to input size helps estimate scalability and identify optimization opportunities.</li><br>
                            <li>Space Complexity Analysis: Evaluating memory usage aids in optimizing memory utilization and minimizing memory footprint.</li><br>
                            <li>Algorithmic Efficiency: Comparing algorithms based on time and space complexity assists in selecting the most suitable algorithm for improved efficiency.</li><br>
                            <li>Profiling and Debugging Tools: Tools like debuggers provide insights into program behavior and resource usage, aiding in identifying bottlenecks and errors.</li><br>
                            <li>Benchmarking: Comparing program performance against standards or competing solutions validates efficiency and guides improvement efforts.</li><br>
                            <li>Code Review and Refactoring: Reviewing code for readability and maintainability, and refactoring for improved design and performance are essential practices.</li><br>
                            <li>Optimization Techniques: Applying algorithmic improvements, selecting appropriate data structures, and leveraging compiler optimizations significantly enhance program performance.</li><br>
                        </ol><br>
                        <p>In program design, modularity is crucial for managing complexity, facilitating focused attention on module details, and simplifying implementation, debugging, and maintenance. Understanding algorithmic complexity in terms of time and space is vital, as it helps evaluate efficiency and resource requirements. Embracing these principles in software development ensures the creation of reliable, scalable, and maintainable software solutions.</p><br>
                    </section>
                    <section>
                        <h2>Arrays:</h2>
                        <p>Arrays are foundational data structures in computer science, storing elements of the same type in contiguous memory locations. They offer efficient access to elements through indices and find wide applications across programming for data organization and manipulation. Arrays come with key features:</p><br>
                        <ol>
                            <li>Ordered Collection: Elements in arrays maintain order based on indices, enabling sequential access. The first element typically starts at index 0.</li><br>
                            <li>Fixed Size: Arrays have a predetermined size upon declaration, which remains constant throughout the program's execution.</li><br>
                            <li>Random Access: Accessing elements in arrays is achieved in constant time by directly referencing their indices.</li><br>
                            <li>Homogeneous Elements: Arrays store elements of the same data type, ensuring consistency and facilitating efficient memory allocation.</li><br>
                            <li>Contiguous Memory Allocation: Elements in arrays are stored in adjacent memory locations, enabling efficient memory access and traversal.</li><br>
                        </ol><br>
                        <p>Common operations on arrays include accessing elements in constant time, insertion and deletion with complexities depending on the position, traversal through sequential visits, and searching using linear or binary search algorithms.</p><br>
                        <p>Applications of arrays span various domains:</p><br>
                        <ol>
                            <li>Data Storage: Arrays store collections of data elements, facilitating easy manipulation and processing.</li><br>
                            <li>Iterative Algorithms: They are integral to iterative algorithms like sorting, searching, and traversing due to their sequential access nature.</li><br>
                            <li>Matrices and Multidimensional Arrays: Arrays represent matrices and multidimensional data structures, allowing efficient manipulation of tabular data.</li><br>
                            <li>Buffers and Caches: In computer systems, arrays are used to implement buffers and caches for storing and processing data efficiently.</li><br>
                        </ol><br>
                        <p>Understanding arrays' properties, operations, and applications is fundamental for developing efficient and scalable software solutions across engineering domains.</p><br>
                    </section>
                    
                    <section>
                        <h2>Hash Tables</h2><br>
                        <p>Hash tables are pivotal data structures that facilitate efficient storage, retrieval, and deletion of key-value pairs, underpinned by hashing principles. They embody several key features:</p><br>
                        <ol>
                            <li><strong>Key-Value Storage:</strong> Data is stored in hash tables as key-value pairs, enabling swift value retrieval based on unique keys.</li><br>
                            <li><strong>Hashing:</strong> Utilizing hash functions, keys are converted into indices within an array, optimizing lookup efficiency.</li><br>
                            <li><strong>Collision Handling:</strong> Collisions, where multiple keys hash to the same index, are managed through strategies like chaining or open addressing.</li><br>
                            <li><strong>Dynamic Sizing:</strong> Hash tables dynamically resize to accommodate varying numbers of stored pairs, ensuring optimal performance as the load factor fluctuates.</li><br>
                        </ol><br>
                        <p>Essential operations on hash tables include insertion, lookup, and deletion, each leveraging the hash function to locate the desired key-value pair efficiently.</p><br>
                        <p>Applications of hash tables span diverse domains:</p><br>
                        <ol>
                            <li><strong>Data Caching:</strong> Employed in caching mechanisms to store frequently accessed data, diminishing the need for resource-intensive computations.</li><br>
                            <li><strong>Symbol Tables:</strong> Integral to compilers, interpreters, and programming languages for efficient storage and retrieval of identifiers and associated data.</li><br>
                            <li><strong>Databases:</strong> Utilized in database indexing and lookup operations to expedite queries and enhance overall database performance.</li><br>
                            <li><strong>Distributed Systems:</strong> In distributed systems, hash tables facilitate data partitioning, load balancing, and distributed caching across multiple nodes, ensuring efficient data storage and retrieval.</li><br>
                        </ol><br>
                        <p>Hash tables, by virtue of their hashing and collision resolution techniques, serve as robust solutions for applications necessitating rapid access to data based on unique keys, thus forming a cornerstone in software engineering and computer science.</p><br>
                    </section>
                    
                    <section>
                        <h2>Concept of Sequential Organization</h2><br>
                        <p>The concept of sequential organization in data structures entails arranging elements in a linear or sequential fashion, facilitating efficient access and manipulation based on their positions or indices within the structure. Key aspects of sequential organization include:</p><br>
                        <ol>
                            <li><strong>Ordering:</strong> Elements maintain a specific order based on their position in the sequence, enabling predictable traversal and retrieval.</li><br>
                            <li><strong>Access by Position:</strong> Direct access to elements is possible through their indices, facilitating efficient traversal and retrieval operations.</li><br>
                            <li><strong>Insertion and Deletion:</strong> Operations involve shifting elements to accommodate changes in the sequence, with considerations for space allocation and preservation of order.</li><br>
                            <li><strong>Traversal:</strong> Sequential traversal entails visiting each element in a sequential manner, typically facilitated by iterative constructs like loops.</li><br>
                            <li><strong>Sequential Search:</strong> Linear search algorithms iterate through elements sequentially to locate a target element within the structure.</li><br>
                        </ol><br>
                        <p>Examples of sequentially organized data structures include arrays, linked lists, queues, and stacks, each serving distinct purposes while adhering to sequential access patterns.</p><br>
                        <p>Arrays represent a classic example, offering efficient sequential access but incurring overhead for insertion and deletion due to shifting requirements. Linked lists, on the other hand, allow for dynamic insertion and deletion without the need for shifting, albeit with potentially slower access times.</p><br>
                        <p>Queues and stacks follow specific access patterns - FIFO (First-In, First-Out) for queues and LIFO (Last-In, First-Out) for stacks - supporting efficient insertion, deletion, and traversal operations.</p><br>
                        <p>Understanding sequential organization principles is foundational to designing and implementing efficient data structures tailored to diverse application requirements and scenarios in software engineering.</p><br>
                    </section>

                    <section>
                        <h2>Linear Data Structures</h2><br>
                        <p><strong>Definition:</strong> Linear data structures organize elements sequentially, with each element connected to its preceding and succeeding elements in a linear manner.</p><br>
                        <p><strong>Examples:</strong> Arrays, stacks, queues, and linked lists.</p><br>
                        <p><strong>Properties:</strong> Linear data structures facilitate straightforward traversal from one end to the other and support operations like insertion and deletion, typically following either LIFO (Last In, First Out) or FIFO (First In, First Out) ordering.</p><br>
                        <p><strong>Applications:</strong> Linear data structures are suitable for scenarios where elements need to be accessed sequentially or stored in an ordered manner, such as managing collections of data or implementing algorithms like sorting and searching.</p><br>
                    </section>
                    
                    <section>
                        <h2>Non-linear Data Structures</h2><br>
                        <p><strong>Definition:</strong> Non-linear data structures do not organize elements sequentially and may exhibit complex relationships between elements, forming networks or hierarchies.</p><br>
                        <p><strong>Examples:</strong> Trees and graphs.</p><br>
                        <p><strong>Properties:</strong> Non-linear data structures represent hierarchical relationships or arbitrary connections between elements, allowing for more flexible data organization and representation.</p><br>
                        <p><strong>Applications:</strong> Non-linear data structures are valuable for modeling various real-world scenarios with complex relationships, such as representing hierarchical data (trees) or network structures (graphs), applicable in domains like social networks, computer networks, and organizational hierarchies.</p><br>
                    </section>
                    
                    <section>
                        <h2>Differences between Linear and Non-linear Data Structures</h2><br>
                        <ul>
                            <li><strong>Organization:</strong> Linear data structures organize elements sequentially, while non-linear data structures exhibit more complex relationships between elements, often forming networks or hierarchies.</li><br>
                            <li><strong>Traversal:</strong> Linear data structures support sequential traversal, whereas non-linear data structures may involve exploring multiple paths or branches due to the presence of multiple connections between elements.</li><br>
                            <li><strong>Access Patterns:</strong> Access patterns differ between linear and non-linear data structures, with linear structures typically supporting direct access using indices or pointers, while non-linear structures may require traversing paths or following relationships between nodes.</li><br>
                        </ul><br>
                        <p>Understanding the characteristics and applications of both linear and non-linear data structures is essential for designing efficient algorithms and solving real-world problems effectively in various engineering and computational domains.</p><br>
                    </section>

                    <section>
                        <h2>Storage Representation</h2><br>
                        <p>Storage representation in data structures is a fundamental concept that dictates how data elements are stored in memory, impacting efficiency and functionality. Different techniques are employed to organize and store data elements, ensuring optimal access, manipulation, and retrieval.</p><br>
                        <p><strong>Key Aspects of Storage Representation:</strong></p><br>
                        <ol>
                            <li>Contiguous Memory Allocation: Arrays and matrices utilize contiguous memory allocation, enabling direct access to elements using indices and supporting efficient traversal and manipulation operations.</li><br>
                            <li>Linked Data Structures: Linked lists, trees, and graphs utilize pointers or references to connect individual elements, forming chains or networks of interconnected nodes.</li><br>
                            <li>Node Structure: Nodes in linked data structures consist of a data field storing the actual element and a pointer/reference field pointing to the next (and possibly previous) node, determining how elements are linked and accessed.</li><br>
                            <li>Dynamic Memory Allocation: Dynamic data structures like dynamic arrays and linked lists support resizing and adaptation to changes in the number of stored elements, optimizing memory usage and flexibility.</li><br>
                            <li>Hashing and Buckets: Hash tables employ hashing techniques to map keys to indices in an array (buckets), facilitating efficient lookup and retrieval of key-value pairs.</li><br>
                            <li>Tree Structures: Trees are hierarchical structures with nodes organized into levels, represented using various storage representations such as arrays, linked lists, or pointers.</li><br>
                            <li>Graph Structures: Graphs consist of vertices and edges, represented using adjacency matrices, lists, or edge lists, each with advantages and trade-offs in memory usage and access efficiency.</li><br>
                        </ol><br>
                        <p><strong>Summary of Concepts:</strong> The distinction between "data structures" and "storage structures" is crucial, emphasizing the importance of thorough initial study of data organization. Various alternative data structures or patterns of mutual accessibility among data elements should be explored independently of specific computer architectures or programming languages. Diagrams and tables can aid in representing data and storage structures, facilitating understanding and analysis.</p><br>
                        <p><strong>Conclusion:</strong> Understanding storage representation is essential for designing efficient data structures that meet application requirements. By selecting appropriate storage representations and optimizing memory usage, developers can efficiently manage and manipulate data elements in memory, contributing to effective problem-solving and application development in engineering contexts.</p><br>
                    </section>
                    
                    <section>
                        <h2>Array Processing Sparse Matrices</h2><br>
                        <p>Sparse matrices, characterized by having a majority of zero elements, pose challenges in terms of memory efficiency and computational complexity when processed using traditional array representations. To address this, specialized data structures and algorithms are utilized, such as compressed sparse row (CSR) and compressed sparse column (CSC) formats.</p><br>
                        <p><strong>Sparse Matrix Representation:</strong> Various formats, including Coordinate List (COO), CSR, and CSC, are used to represent sparse matrices, optimizing storage and operation efficiency.</p><br>
                        <p><strong>Array Processing:</strong> Efficient algorithms tailored for sparse matrix representations are employed for operations like addition, multiplication, and transposition, leveraging sparsity to minimize memory usage and computational overhead.</p><br>
                        <p><strong>Sparse Matrix-Vector Multiplication (SpMV):</strong> This operation is fundamental in scientific and engineering applications, involving multiplication of a sparse matrix by a dense vector. Algorithms like CSR-based SpMV exploit matrix sparsity to reduce arithmetic operations and memory accesses.</p><br>
                        <p><strong>Sparse Matrix-Matrix Multiplication (SpMM):</strong> Another crucial operation in numerical computing, SpMM involves multiplying two sparse matrices to produce another sparse matrix. Algorithms like CSR-based SpGEMM minimize memory usage and computational complexity.</p><br>
                        <p><strong>Iterative Solvers:</strong> Sparse matrices often arise in solving large systems of linear equations. Iterative solvers like Conjugate Gradient (CG) and Generalized Minimal RESidual (GMRES) methods efficiently solve sparse linear systems by leveraging matrix sparsity.</p><br>
                        <p><strong>Sparse Matrix Libraries:</strong> Various software libraries like SuiteSparse, Intel Math Kernel Library (MKL), and SciPy provide efficient implementations of sparse matrix data structures and algorithms, easing development tasks.</p><br>
                        <p>Sparse matrices, despite their computational challenges, are efficiently processed using specialized data structures and algorithms, enabling storage, manipulation, and computation on large-scale sparse datasets in various domains.</p><br>
                        <p>The input also provides a detailed explanation of sparse matrix representation, including array and linked list representations in programming languages like C and Java. Sparse matrix representation reduces memory usage and computational complexity by storing only non-zero elements, facilitating efficient traversal and manipulation. Various examples illustrate the benefits and implementations of sparse matrix representation using both array and linked list structures.</p><br>
                    </section>

                    <section>
                        <h2>Transpose of Sparse Matrices</h2><br>
                        <p>The transpose of a matrix involves swapping its rows and columns. For sparse matrices, which have mostly zero elements, specialized algorithms preserve sparsity and minimize computational overhead during the transpose operation.</p><br>
                        <p><strong>Key Aspects of Transpose of Sparse Matrices:</strong></p><br>
                        <ol>
                            <li>Sparsity Preservation: Efficient algorithms maintain the sparsity structure of sparse matrices, ensuring that only non-zero elements are transposed to avoid introducing unnecessary elements.</li><br>
                            <li>Transpose Representation: Sparse matrices are typically represented using optimized data structures. The same data structure can often be reused with slight modifications to represent the transposed matrix.</li><br>
                            <li>Compressed Sparse Row (CSR) Format: Transposing a sparse matrix in the CSR format involves swapping row and column indices of non-zero elements while adjusting auxiliary arrays like row indices and column pointers.</li><br>
                            <li>Compressed Sparse Column (CSC) Format: Similar to CSR, transposing a sparse matrix in the CSC format entails swapping column and row indices of non-zero elements, with adjustments to auxiliary arrays.</li><br>
                            <li>Efficient Algorithms: Algorithms for computing the transpose of sparse matrices minimize memory usage and computational complexity. They exploit matrix sparsity to avoid unnecessary operations on zero elements and optimize memory access patterns.</li><br>
                            <li>Applications: The transpose of sparse matrices finds utility in numerical and scientific computing applications, including solving linear equations, matrix-vector and matrix-matrix multiplications, and optimization problems. Efficient computation of the transpose enables faster and more memory-efficient execution of these operations.</li><br>
                            <li>Sparse Matrix Libraries: Various software libraries offer efficient implementations of transpose algorithms for sparse matrices, facilitating optimized functions and routines for computing transposes in different programming languages and environments.</li><br>
                        </ol><br>
                        <p>Efficient computation of the transpose of sparse matrices is crucial for numerous numerical computations and scientific simulations involving large-scale sparse datasets. Specialized algorithms and data structures tailored for sparse matrices enable developers to perform transpose operations with minimal memory usage and computational overhead, facilitating faster and more scalable computations across diverse application domains.</p><br>
                        <p>Moreover, the provided implementation in C++ demonstrates operations like addition, multiplication, and transpose on sparse matrices. It includes methods for inserting elements into sparse matrices, adding matrices, computing the transpose, and multiplying matrices efficiently. The worst-case time complexities for these operations are also discussed, providing insights into their computational efficiency.</p><br>
                    </section>

                    <section>
                        <h2>Hash Tables</h2><br>
                        <h3>Direct Address Tables:</h3><br>
                        <p><strong>Definition:</strong><br>
                            Direct address tables are a type of hash table where the array index is directly determined from the key. This is a straightforward method that uses the key as the index in an array.
                        </p><br>
                        <p><strong>Implementation:</strong><br>
                            An array is initialized with a size equal to the range of possible keys. Each element in the array can store a value associated with a key. The key itself is used as the index in the array to directly access or modify the value.
                        </p><br>
                        <p><strong>Advantages:</strong><br>
                            Direct access: Since keys directly correspond to array indices, accessing a value using a key is very fast (O(1) time complexity).<br>
                            Simplicity: The implementation is straightforward and easy to understand.
                        </p><br>
                        <p><strong>Disadvantages:</strong><br>
                            Wasted space: If the range of possible keys is large and the number of actual keys used is small, a direct address table can consume a lot of memory.<br>
                            Limited key range: The method is only feasible if the range of possible keys is small and dense.
                        </p><br>
                        <p><strong>Applications:</strong><br>
                            Direct address tables are suitable for applications where keys are densely packed within a known, small range. Common in scenarios such as maintaining a count of distinct elements in a small range, or mapping user IDs to data in a known range.
                        </p><br>
                        <br>
                        <h3>Hash Functions:</h3><br>
                        <p>Hash functions play a vital role in hash table implementations, converting input keys into fixed-size integers called hash codes. These hash codes are used to efficiently map keys to indices in an array, facilitating fast retrieval and storage of key-value pairs. Key aspects of hash functions include determinism, fixed output size, uniform distribution, collision resistance, fast computation, and the avalanche effect. Commonly used hash functions include the division method, multiplication method, and universal hashing.</p><br>
                        <p>Other hash function types discussed include the mid-square method, digit folding method, and various cryptographic hash functions like SHA, MD5, and BLAKE2. These hash functions serve different purposes, from error detection in data transmission (CRC) to cryptographic security (SHA, MD5). Each hash function has its pros and cons, with considerations such as performance, collision resistance, and memory usage.</p><br>
                        <p>In summary, hash functions are essential components of hash table implementations, providing efficient mapping of keys to indices and enabling fast retrieval and storage of key-value pairs. Understanding different hash function types and their properties is crucial for designing optimized data structures and ensuring the security and integrity of digital information.</p><br>
                    </section>

                    <section>
                        <h3>Open addressing:</h3><br>
                        <p><strong>Definition:</strong><br>
                            Open addressing is a collision resolution technique used in hash tables where, upon encountering a collision, the algorithm searches for an available slot using a probing strategy.
                        </p><br>
                        <p><strong>Techniques:</strong><br>
                            <strong>Linear Probing:</strong><br>
                            When a collision occurs, the next available slot in the hash table is checked sequentially. This can lead to clustering, where groups of keys are mapped to neighboring indices.<br>
                            <strong>Quadratic Probing:</strong><br>
                            Probing for the next available slot is done using a quadratic function. For example, if the original hash index is `i`, the next index to probe can be calculated as `(i + j^2) % table_size`, where `j` is the probing iteration.<br>
                            <strong>Double Hashing:</strong><br>
                            Uses a second hash function to determine the step size for probing. This technique helps reduce the likelihood of clustering and can improve performance.
                        </p><br>
                        <p><strong>Advantages:</strong><br>
                            Better utilization of table space: Open addressing can use all slots in the hash table.<br>
                            Avoids chaining: Instead of creating separate lists for each key, open addressing keeps all keys in the same array.
                        </p><br>
                        <p><strong>Disadvantages:</strong><br>
                            Primary and secondary clustering: Linear probing can result in clustering, which can affect performance.<br>
                            Limited load factor: Open addressing generally performs well only when the hash table is not heavily loaded.
                        </p><br>
                        <p><strong>Example:</strong><br>
                            In double hashing, the first hash function could be `hash1 = key % table_size` and the second hash function could be `hash2 = 1 + (key % (table_size - 1))`. The probing index would then be calculated as `(hash1 + j * hash2) % table_size`, where `j` is the probing iteration.
                        </p><br>
                        <br>
                        <h3>Perfect hashing:</h3><br>
                        <p>Perfect hashing is an advanced technique utilized to minimize collisions in hash tables, ensuring each key is uniquely mapped to a distinct index. Unlike traditional hash tables employing collision resolution techniques, perfect hashing eliminates collisions entirely, providing constant-time access to key-value pairs.</p><br>
                        <p>A two-level hashing process characterizes perfect hashing, employing primary and secondary hash functions. The primary hash function determines a bucket for each key, while the secondary hash function ensures uniqueness within each bucket. This approach aims to distribute keys evenly across buckets, minimizing collisions.</p><br>
                        <p>Perfect hashing can be static or dynamic, accommodating fixed or changing sets of keys. Static perfect hashing suits scenarios with a fixed set of keys, while dynamic perfect hashing handles dynamic changes in key sets.</p><br>
                        <p>Construction algorithms like the FKS and Cuckoo hashing algorithms minimize collisions and optimize space and time complexity. Perfect hashing finds application in various domains requiring efficient access to key-value pairs, such as compilers, database indexing, and dictionaries.</p><br>
                        <p>Benefits of perfect hashing include constant-time access, minimal memory overhead by eliminating collision resolution techniques, and optimal performance for hash table operations.</p><br>
                        <p>Implementation involves two hash tables, each utilizing universal hashing. The first level hashes elements into slots, while the second level stores elements hashing to the same slot in a secondary hash table. The size of the secondary table ensures no collisions.</p><br>
                        <p>Choosing hash functions involves selecting from universal hash families to limit collisions. The probability of collisions is minimized through careful selection of hash functions.</p><br>
                        <p>In analysis, when the number of keys is small, a single-level hash table suffices, but for larger sets, two-level hashing is employed. The first level hashes keys into slots, while the second level utilizes a secondary hash table to prevent collisions, ensuring efficient storage and retrieval of key-value pairs.</p><br>
                    </section>
                    
                </div>

                <div id="unit2" class="formatted-text">
                    <!-- Unit 2 text -->
                    <h2>UNIT 2: Stacks and Queues</h2>
                    <br>
                    <section>
                        <h2>Introduction</h2><br>
                        <p>Stacks and queues are essential data structures used in computer science for organizing and managing data in a specific order to facilitate efficient access and manipulation.</p><br>
                        <p>Stacks follow the Last In, First Out (LIFO) principle, where the last element added is the first to be removed. Operations such as push (adding an element), pop (removing and returning the top element), and peek (viewing the top element) are supported. Stacks are commonly used in applications involving recursive function calls, expression evaluation, backtracking, and undo functionality.</p><br>
                        <p>On the other hand, queues adhere to the First In, First Out (FIFO) principle, meaning the first element added is the first to be removed. Operations like enqueue (adding an element to the rear), dequeue (removing and returning the front element), and peek (viewing the front element) are supported. Queues find applications in scheduling, task management, breadth-first search algorithms, and buffering of data.</p><br>
                        <p>Both stacks and queues are abstract data types (ADTs) that organize data based on specific rules (LIFO for stacks and FIFO for queues). They can be implemented using various underlying data structures like arrays, linked lists, or dynamic arrays.</p><br>
                        <p>The main difference between stacks and queues lies in their order of insertion and removal of elements. Stacks are ideal for tasks requiring last-in, first-out behavior, while queues are suitable for first-in, first-out scenarios. Understanding the concepts and operations of stacks and queues is crucial for designing and implementing efficient algorithms and data structures in various applications. Leveraging their unique properties enables developers to solve a wide range of problems efficiently and effectively.</p><br>
                    </section>
                    
                    <section>
                        <h2>Stack and Queue as ADT</h2><br>
                        <p><strong>Introduction to Stack and Queue as Abstract Data Types (ADTs)</strong></p><br>
                        <h3>Stacks:</h3><br>
                        <ul>
                            <li><strong>Definition:</strong><br>
                                <ul>
                                    <li>A stack is a linear data structure that follows the Last In, First Out (LIFO) order.</li><br>
                                    <li>It allows insertion and deletion of elements only from one end, called the top.</li><br>
                                </ul>
                            </li><br><br>
                            <li><strong>Operations:</strong><br>
                                <ul>
                                    <li>Push: Inserts an element onto the top of the stack.</li><br>
                                    <li>Pop: Removes the top element from the stack.</li><br>
                                    <li>Peek/Top: Returns the top element of the stack without removing it.</li><br>
                                    <li>IsEmpty: Checks if the stack is empty.</li><br>
                                </ul>
                            </li><br><br>
                            <li><strong>Applications:</strong><br>
                                <ul>
                                    <li>Used in expression evaluation (e.g., infix to postfix conversion).</li><br>
                                    <li>Can be used to reverse data or control function calls in recursive programming.</li><br>
                                </ul>
                            </li><br><br>
                        </ul><br>
                        <h3>Queues:</h3><br>
                        <ul>
                            <li><strong>Definition:</strong><br>
                                <ul>
                                    <li>A queue is a linear data structure that follows the First In, First Out (FIFO) order.</li><br>
                                    <li>It allows insertion of elements at the rear and removal of elements from the front.</li><br>
                                </ul>
                            </li><br><br>
                            <li><strong>Operations:</strong><br>
                                <ul>
                                    <li>Enqueue: Inserts an element at the rear of the queue.</li><br>
                                    <li>Dequeue: Removes the front element from the queue.</li><br>
                                    <li>Front/Peek: Returns the front element without removing it.</li><br>
                                    <li>IsEmpty: Checks if the queue is empty.</li><br>
                                </ul>
                            </li><br><br>
                            <li><strong>Applications:</strong><br>
                                <ul>
                                    <li>Used in scheduling and managing processes in operating systems.</li><br>
                                    <li>Useful in breadth-first search (BFS) in graphs.</li><br>
                                </ul>
                            </li><br><br>
                        </ul><br>
                        <p>Both stacks and queues can be implemented using arrays or linked lists, depending on the specific use case and performance considerations.</p><br>
                    </section>

                    <section>
                        <h2>Representation and Implementation of Stack and Queue Using Sequential Allocation</h2><br>
                        <p><strong>Sequential Allocation:</strong> Sequential allocation involves representing stacks and queues using contiguous memory blocks, typically arrays.</p><br>
                        
                        <h3>Stack Implementation:</h3><br>
                        <p><strong>Array-Based Stack:</strong></p><br>
                        <ul>
                            <li>The stack is represented using an array.</li><br>
                            <li>A variable (often called <code>top</code>) keeps track of the index of the top element in the stack.</li><br>
                        </ul><br>
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Push:</strong> Check if the stack is full. If not, increment <code>top</code> and insert the new element at the new top position.</li><br>
                            <li><strong>Pop:</strong> Check if the stack is empty. If not, remove the element at the top and decrement <code>top</code>.</li><br>
                            <li><strong>Peek/Top:</strong> Return the element at the top position without removing it.</li><br>
                        </ul><br>
                        <p><strong>Advantages:</strong> Fast access to the top element (O(1)). Simple and efficient implementation.</p><br>
                        <p><strong>Disadvantages:</strong> Fixed-size array limits stack size, which may cause overflow.</p><br>
                        
                        <h3>Queue Implementation:</h3><br>
                        <p><strong>Array-Based Queue:</strong></p><br>
                        <ul>
                            <li>The queue is represented using an array.</li><br>
                            <li>Variables (<code>front</code> and <code>rear</code>) track the indices of the front and rear of the queue.</li><br>
                        </ul><br>
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Enqueue:</strong> Check if the queue is full. If not, insert the new element at the rear position and increment <code>rear</code>.</li><br>
                            <li><strong>Dequeue:</strong> Check if the queue is empty. If not, remove the element at the front and increment <code>front</code>.</li><br>
                            <li><strong>Front/Peek:</strong> Return the element at the front position without removing it.</li><br>
                        </ul><br>
                        <p><strong>Advantages:</strong> Fast access to the front and rear elements (O(1)). Simple implementation.</p><br>
                        <p><strong>Disadvantages:</strong> Fixed-size array limits queue size. Linear queue may face issues such as "queue overflow" and "queue underflow."</p><br>
                        
                        <p>Sequential allocation provides a straightforward and efficient method to implement stacks and queues, but fixed-size arrays can be limiting in terms of capacity.</p><br>
                    </section>

                    <section>
                        <h2>Representation and Implementation of Stack and Queue Using Linked Allocation</h2><br>
                        <p><strong>Linked Allocation:</strong> Linked allocation involves representing stacks and queues using linked lists rather than arrays. This allows for dynamic resizing of the data structures.</p><br>
                        
                        <h3>Stack Implementation:</h3><br>
                        <p><strong>Linked List-Based Stack:</strong></p><br>
                        <ul>
                            <li>The stack is implemented using a linked list.</li><br>
                            <li>The <code>top</code> variable points to the head of the linked list.</li><br>
                        </ul><br>
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Push:</strong> Create a new node with the value to be pushed. Set the new node's next pointer to the current <code>top</code>. Update <code>top</code> to point to the new node.</li><br>
                            <li><strong>Pop:</strong> Check if the stack is empty. If not, remove the node at the <code>top</code>. Update <code>top</code> to point to the next node.</li><br>
                            <li><strong>Peek/Top:</strong> Return the value of the node at the <code>top</code>.</li><br>
                        </ul><br>
                        <p><strong>Advantages:</strong> No fixed size; stack can grow or shrink dynamically. Efficient use of memory, as the stack only uses as much space as needed.</p><br>
                        <p><strong>Disadvantages:</strong> Slightly slower access due to linked list traversal (O(1) for operations).</p><br>
                        
                        <h3>Queue Implementation:</h3><br>
                        <p><strong>Linked List-Based Queue:</strong></p><br>
                        <ul>
                            <li>The queue is implemented using a linked list.</li><br>
                            <li><code>Front</code> points to the head of the linked list, and <code>rear</code> points to the tail.</li><br>
                        </ul><br>
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Enqueue:</strong> Create a new node with the value to be enqueued. Set the rear node's next pointer to the new node. Update <code>rear</code> to point to the new node.</li><br>
                            <li><strong>Dequeue:</strong> Check if the queue is empty. If not, remove the node at the <code>front</code>. Update <code>front</code> to point to the next node.</li><br>
                            <li><strong>Front/Peek:</strong> Return the value of the node at the <code>front</code>.</li><br>
                        </ul><br>
                        <p><strong>Advantages:</strong> No fixed size; queue can grow or shrink dynamically. Efficient use of memory, as the queue only uses as much space as needed.</p><br>
                        <p><strong>Disadvantages:</strong> Slightly slower access due to linked list traversal (O(1) for operations). Requires additional memory for pointers.</p><br>
                        
                        <p>Using linked allocation allows for dynamic resizing of stacks and queues, enabling the data structures to grow or shrink as needed.</p><br>
                    </section>

                    <section>
                        <h2>Circular Queue and its Implementation</h2><br>
                        <p><strong>Definition:</strong></p><br>
                        <ul>
                            <li>A circular queue, also known as a ring buffer, is a linear data structure that follows the FIFO (First In, First Out) principle but wraps around upon reaching the end of the array.</li><br>
                            <li>Circular queues are implemented using arrays.</li><br>
                        </ul><br>
                        
                        <p><strong>Implementation:</strong></p><br>
                        <ul>
                            <li>Circular queues use two pointers, <code>front</code> and <code>rear</code>, to track the front and rear of the queue, respectively.</li><br>
                            <li>When <code>rear</code> or <code>front</code> reaches the end of the array, they wrap around to the beginning, forming a circular structure.</li><br>
                        </ul><br>
                        
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Enqueue:</strong> Check if the queue is full by comparing <code>(rear + 1) % queue_size == front</code>. If not full, insert the element at <code>rear</code>. Update <code>rear</code> using <code>(rear + 1) % queue_size</code>.</li><br>
                            <li><strong>Dequeue:</strong> Check if the queue is empty by comparing <code>front == rear</code>. If not empty, remove the element at <code>front</code>. Update <code>front</code> using <code>(front + 1) % queue_size</code>.</li><br>
                            <li><strong>Front/Peek:</strong> Return the value at the <code>front</code> position without removing it.</li><br>
                        </ul><br>
                        
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>Efficient Use of Space: Allows efficient use of the entire array by wrapping around.</li><br>
                            <li>Constant Time Operations: Enqueue and dequeue operations can be performed in constant time (O(1)).</li><br>
                        </ul><br>
                        
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>Limited Size: The queue has a fixed size determined by the size of the array.</li><br>
                            <li>Implementation Complexity: Managing the wrap-around logic can introduce complexity in the implementation.</li><br>
                        </ul><br>
                        
                        <p>Circular queues are useful in scenarios where memory needs to be efficiently utilized and when the circular structure provides convenience for operations such as cycling through a set of items.</p><br>
                    </section>
                    
                    <section>
                        <h2>Application of Stack for Expression Evaluation and Expression Conversion</h2><br>
                        
                        <h3>Expression Evaluation:</h3><br>
                        <p><strong>Infix Notation:</strong> An expression where operators are placed between operands (e.g., <code>A + B</code>).</p><br>
                        <p><strong>Postfix Notation (Reverse Polish Notation):</strong> An expression where operators follow the operands (e.g., <code>A B +</code>). Evaluated using a stack.</p><br>
                        <p><strong>Prefix Notation (Polish Notation):</strong> An expression where operators precede the operands (e.g., <code>+ A B</code>). Also evaluated using a stack.</p><br>
                        
                        <p><strong>Using Stacks for Evaluation:</strong></p><br>
                        <ul>
                            <li><strong>Postfix Evaluation:</strong> Traverse the postfix expression from left to right. If an operand is encountered, push it onto the stack. If an operator is encountered, pop the necessary operands from the stack, apply the operator, and push the result back onto the stack. The final result will be at the top of the stack.</li><br>
                            <li><strong>Prefix Evaluation:</strong> Traverse the prefix expression from right to left. If an operand is encountered, push it onto the stack. If an operator is encountered, pop the necessary operands from the stack, apply the operator, and push the result back onto the stack. The final result will be at the top of the stack.</li><br>
                        </ul><br>
                        
                        <h3>Expression Conversion:</h3><br>
                        <p><strong>Infix to Postfix Conversion:</strong> Use a stack to manage operators and their precedence. Traverse the infix expression from left to right. If an operand is encountered, append it to the output. If an operator is encountered, pop operators from the stack based on precedence and associativity and append them to the output. Push the current operator onto the stack. Continue until the expression is processed. At the end, pop any remaining operators from the stack and append them to the output.</p><br>
                        <p><strong>Infix to Prefix Conversion:</strong> Convert the infix expression to a reverse Polish notation (postfix). Reverse the order of operands and operators in the postfix expression to obtain the prefix expression.</p><br>
                        
                        <p><strong>Advantages of Using Stacks:</strong></p><br>
                        <ul>
                            <li>Stacks provide an efficient way to evaluate and convert expressions using their LIFO order.</li><br>
                            <li>The stack handles operators and operands appropriately based on precedence and associativity.</li><br>
                        </ul><br>
                    </section>

                    <section>
                        <h2>Recursion</h2><br>
                        <p><strong>Definition:</strong></p><br>
                        <ul>
                            <li>Recursion is a programming technique where a function calls itself directly or indirectly to solve a problem.</li><br>
                            <li>A recursive function consists of a base case and one or more recursive cases.</li><br>
                        </ul><br>
                        
                        <p><strong>Key Concepts:</strong></p><br>
                        <ul>
                            <li><strong>Base Case:</strong> The condition that stops the recursion. This is essential to prevent infinite recursion.</li><br>
                            <li><strong>Recursive Case:</strong> The function calls itself with a smaller or simpler input until it reaches the base case.</li><br>
                        </ul><br>
                        
                        <p><strong>How Recursion Works:</strong></p><br>
                        <ul>
                            <li>When a recursive function is called, it creates a new stack frame on the call stack.</li><br>
                            <li>Each stack frame contains the function’s local variables and return address.</li><br>
                            <li>When the base case is reached, the function begins to return and the call stack unwinds.</li><br>
                        </ul><br>
                        
                        <p><strong>Examples:</strong></p><br>
                        <ul>
                            <li><strong>Factorial:</strong> The factorial function can be defined recursively as <code>n! = n * (n - 1)!</code> with a base case of <code>0! = 1</code>.</li><br>
                            <li><strong>Fibonacci:</strong> The Fibonacci sequence can be defined recursively as <code>F(n) = F(n - 1) + F(n - 2)</code> with base cases <code>F(0) = 0</code> and <code>F(1) = 1</code>.</li><br>
                        </ul><br>
                        
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>Simplicity: Recursion can lead to clean and elegant solutions for problems that have a naturally recursive structure.</li><br>
                            <li>Code Reusability: Recursive functions can often be reused for similar problems.</li><br>
                        </ul><br>
                        
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>Memory Usage: Each recursive call consumes stack space, potentially leading to stack overflow for deep recursions.</li><br>
                            <li>Performance: Recursive solutions can be less efficient than iterative solutions due to overhead from repeated function calls.</li><br>
                        </ul><br>
                        
                        <p><strong>Tail Recursion:</strong></p><br>
                        <ul>
                            <li>A specific form of recursion where the recursive call is the last operation in the function.</li><br>
                            <li>Some languages can optimize tail recursion to prevent excessive stack usage.</li><br>
                        </ul><br>
                        
                        <p><strong>Applications:</strong></p><br>
                        <ul>
                            <li>Used in various problems such as tree traversals, divide-and-conquer algorithms, and backtracking.</li><br>
                        </ul><br>
                    </section>
                    
                    <section>
                        <h2>Priority Queue</h2><br>
                        <p><strong>Definition:</strong></p><br>
                        <ul>
                            <li>A priority queue is a data structure similar to a regular queue, but each element has an associated priority.</li><br>
                            <li>Elements with higher priority are served before elements with lower priority.</li><br>
                        </ul><br>
                        
                        <p><strong>Implementation:</strong></p><br>
                        <ul>
                            <li><strong>Array/List:</strong> Elements can be stored in an array or list and sorted by priority. Insertion and deletion may require reordering, which can be inefficient.</li><br>
                            <li><strong>Heap:</strong> A heap, often a binary heap, is a popular choice for implementing priority queues. Provides efficient insertion, deletion, and retrieval of the highest priority element. Heaps allow for operations in O(log n) time complexity.</li><br>
                            <li><strong>Binary Search Tree:</strong> Elements are stored in a tree and prioritized based on key values. This can allow efficient operations but may require balancing for optimal performance.</li><br>
                        </ul><br>
                        
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Insert/Enqueue:</strong> Add an element to the queue with a given priority.</li><br>
                            <li><strong>Delete/Dequeue:</strong> Remove the element with the highest priority (e.g., maximum or minimum).</li><br>
                            <li><strong>Peek:</strong> Retrieve the element with the highest priority without removing it.</li><br>
                        </ul><br>
                        
                        <p><strong>Applications:</strong></p><br>
                        <ul>
                            <li>Task Scheduling: Used in operating systems and process management to prioritize tasks.</li><br>
                            <li>Dijkstra's Algorithm: Used in graph algorithms to find the shortest path.</li><br>
                            <li>Event-Driven Programming: Used to manage events based on priority.</li><br>
                        </ul><br>
                        
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>Efficient retrieval of the highest priority element.</li><br>
                            <li>Supports dynamic priority changes for elements.</li><br>
                        </ul><br>
                        
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>May require complex data structures (e.g., heaps) for efficient operations.</li><br>
                            <li>Maintaining the structure can add overhead.</li><br>
                        </ul><br>
                        
                        <p>Priority queues are useful in scenarios where tasks or elements must be managed based on importance or urgency.</p><br>
                    </section>
                    
                </div>

                <div id="unit3" class="formatted-text">
                    <!-- Unit 3 text -->
                    <h2>UNIT 3: Linked list</h2>
                    <br>
                    <section>
                        <h2>Concept of Linked Organization</h2><br>
                        <p><strong>Definition:</strong></p><br>
                        <ul>
                            <li>Linked organization is a method of organizing data where elements are stored as nodes, each containing a data value and a reference (or pointer) to the next node.</li><br>
                            <li>Unlike arrays, linked structures do not require contiguous memory allocation, allowing for dynamic resizing.</li><br>
                        </ul><br>
                        
                        <p><strong>Types of Linked Lists:</strong></p><br>
                        <ul>
                            <li><strong>Singly Linked List:</strong> Nodes contain data and a single reference to the next node.</li><br>
                            <li><strong>Doubly Linked List:</strong> Nodes contain data, a reference to the next node, and a reference to the previous node.</li><br>
                            <li><strong>Circular Linked List:</strong> Nodes are arranged in a circular fashion, where the last node points back to the first node.</li><br>
                        </ul><br>
                        
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>Dynamic resizing: The linked list can grow or shrink dynamically as elements are added or removed.</li><br>
                            <li>Efficient insertion and deletion: Operations such as insertion and deletion can be efficient, especially when done at the beginning or end of the list.</li><br>
                            <li>Flexible memory usage: Linked lists use memory efficiently since they allocate memory only when needed.</li><br>
                        </ul><br>
                        
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>Extra memory for pointers: Each node requires additional memory for storing pointers, which can be inefficient in terms of memory usage.</li><br>
                            <li>Sequential access: Accessing elements requires traversal from the beginning, resulting in slower search times.</li><br>
                        </ul><br>
                        
                        <p><strong>Operations on Linked Lists:</strong></p><br>
                        <ul>
                            <li><strong>Insertion:</strong> Adding a new node to the list.</li><br>
                            <li><strong>Deletion:</strong> Removing a node from the list.</li><br>
                            <li><strong>Traversal:</strong> Accessing and processing each node in the list sequentially.</li><br>
                        </ul><br>
                        
                        <p><strong>Applications:</strong></p><br>
                        <ul>
                            <li>Linked lists are widely used in data storage and manipulation, such as in implementing stacks and queues, dynamic memory management, and adjacency lists in graphs.</li><br>
                        </ul><br>
                        
                        <p>Linked organization forms the basis for more complex data structures such as trees and graphs. It is an essential concept for understanding dynamic data structures in computer engineering.</p><br>
                    </section>

                    <section>
                        <h2>Singly Linked List</h2><br>
                        <p><strong>Definition:</strong></p><br>
                        <ul>
                            <li>A singly linked list is a linear data structure consisting of nodes. Each node contains a data element and a reference (or pointer) to the next node in the list.</li><br>
                        </ul><br>
                        
                        <p><strong>Structure:</strong></p><br>
                        <ul>
                            <li><strong>Node:</strong> The basic building block of a singly linked list, consisting of data and a pointer to the next node.</li><br>
                            <li><strong>Head:</strong> The first node in the list.</li><br>
                            <li><strong>Tail:</strong> The last node in the list, with its next pointer set to <code>null</code>.</li><br>
                        </ul><br>
                        
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Insertion:</strong> Adding a new node at the beginning, end, or a specific position in the list.</li><br>
                            <li><strong>Deletion:</strong> Removing a node from the list.</li><br>
                            <li><strong>Traversal:</strong> Iterating through the list from the head to the tail to access data elements.</li><br>
                        </ul><br>
                        
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>Dynamic resizing: The list can grow or shrink as needed without a predetermined size limit.</li><br>
                            <li>Efficient insertions and deletions: Particularly at the beginning and end of the list.</li><br>
                        </ul><br>
                        
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>Sequential access: Accessing elements requires traversal from the head, making it slower than direct access in arrays.</li><br>
                            <li>No backward traversal: Only supports forward traversal, which can make certain operations (e.g., reverse traversal) complex.</li><br>
                        </ul><br>
                    </section><br>
                    
                    <section>
                        <h2>Doubly Linked List</h2><br>
                        <p><strong>Definition:</strong></p><br>
                        <ul>
                            <li>A doubly linked list is a linear data structure consisting of nodes. Each node contains a data element, a reference (or pointer) to the next node, and a reference to the previous node in the list.</li><br>
                        </ul><br>
                        
                        <p><strong>Structure:</strong></p><br>
                        <ul>
                            <li><strong>Node:</strong> The basic building block of a doubly linked list, consisting of data and pointers to both the next and previous nodes.</li><br>
                            <li><strong>Head:</strong> The first node in the list.</li><br>
                            <li><strong>Tail:</strong> The last node in the list, with its next pointer set to <code>null</code>.</li><br>
                        </ul><br>
                        
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Insertion:</strong> Adding a new node at the beginning, end, or a specific position in the list.</li><br>
                            <li><strong>Deletion:</strong> Removing a node from the list.</li><br>
                            <li><strong>Traversal:</strong> Iterating through the list in both forward and backward directions.</li><br>
                        </ul><br>
                        
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>Bidirectional traversal: Supports traversal in both forward and backward directions.</li><br>
                            <li>Efficient insertions and deletions: Easier insertion and deletion due to access to both previous and next nodes.</li><br>
                        </ul><br>
                        
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>Extra memory for pointers: Each node requires additional memory for storing both next and previous pointers.</li><br>
                            <li>Slightly more complex: Implementation and maintenance can be more complex than singly linked lists.</li><br>
                        </ul><br>
                    </section>

                    <section>
                        <h2>Dynamic Storage Management</h2><br>
                        <p><strong>Definition:</strong></p><br>
                        <ul>
                            <li>Dynamic storage management refers to the process of allocating and deallocating memory during runtime. It allows data structures like linked lists to adjust their size dynamically based on the program's needs.</li><br>
                        </ul><br>
                        
                        <p><strong>Key Concepts:</strong></p><br>
                        <ul>
                            <li><strong>Memory Allocation:</strong> Heap Memory</li><br>
                            <li><strong>Memory Deallocation:</strong> Proper deallocation is essential to prevent memory leaks.</li><br>
                        </ul><br>
                        
                        <p><strong>Benefits:</strong></p><br>
                        <ul>
                            <li>Efficient Memory Use: Only allocates memory when necessary, reducing overall memory usage.</li><br>
                            <li>Flexibility: Data structures like linked lists can dynamically grow or shrink based on data insertion and deletion.</li><br>
                        </ul><br>
                        
                        <p><strong>Challenges:</strong></p><br>
                        <ul>
                            <li>Memory Leaks: Occur when allocated memory is not deallocated, causing the program to consume more memory than necessary.</li><br>
                            <li>Fragmentation: Memory fragmentation can occur if deallocation results in scattered free blocks of memory, potentially affecting performance.</li><br>
                            <li>Garbage Collection: In languages with garbage collection (e.g., Java), the runtime system automatically manages memory allocation and deallocation. However, this can add overhead.</li><br>
                        </ul><br>
                        
                        <p><strong>Best Practices:</strong></p><br>
                        <ul>
                            <li>Efficient Allocation and Deallocation</li><br>
                            <li>Monitoring Memory Usage</li><br>
                            <li>Minimizing Fragmentation</li><br>
                        </ul><br>
                    </section><br>
                    
                    <section>
                        <h2>Circular Linked List</h2><br>
                        <p><strong>Definition:</strong></p><br>
                        <ul>
                            <li>A circular linked list is a data structure where the nodes form a circle. This means that the last node's next pointer points back to the first node, creating a loop.</li><br>
                        </ul><br>
                        
                        <p><strong>Types:</strong></p><br>
                        <ul>
                            <li><strong>Singly Circular Linked List:</strong> A singly circular linked list is similar to a singly linked list, but the last node points back to the first node.</li><br>
                            <li><strong>Doubly Circular Linked List:</strong> A doubly circular linked list is similar to a doubly linked list, but both the last node's next pointer and the first node's previous pointer point to each other, forming a circular structure.</li><br>
                        </ul><br>
                        
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Insertion:</strong> At the beginning, end, or a specific position.</li><br>
                            <li><strong>Deletion:</strong> From the beginning, end, or a specific position.</li><br>
                            <li><strong>Traversal:</strong> Traversal can be done starting from any node and continuing through the circular structure.</li><br>
                        </ul><br>
                        
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>No clear beginning or end: This can be useful in applications that require circular traversal.</li><br>
                            <li>Efficient operations: Insertions and deletions at the head and tail are efficient.</li><br>
                        </ul><br>
                        
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>Complexity: Circular lists can be more complex to implement and maintain compared to linear lists.</li><br>
                            <li>Infinite loop risk: Care must be taken to avoid infinite loops during traversal and operations.</li><br>
                        </ul><br>
                    </section>

                    <section>
                        <h2>Insertion of Linked List</h2><br>
                        <p><strong>Types of Insertion:</strong></p><br>
                        <ul>
                            <li><strong>At the Beginning:</strong></li><br>
                            <ul>
                                <li><strong>Singly Linked List:</strong> Create a new node, set its next pointer to the current head, update the head to point to the new node.</li><br>
                                <li><strong>Doubly Linked List:</strong> Create a new node, set its next pointer to the current head, set the current head's previous pointer to the new node (if it exists), update the head to point to the new node.</li><br>
                            </ul><br>
                            <li><strong>At the End:</strong></li><br>
                            <ul>
                                <li><strong>Singly Linked List:</strong> Create a new node, traverse to the end of the list, set the last node's next pointer to the new node.</li><br>
                                <li><strong>Doubly Linked List:</strong> Create a new node, set its previous pointer to the current tail, update the current tail's next pointer to the new node, update the tail to point to the new node.</li><br>
                            </ul><br>
                            <li><strong>At a Specific Position:</strong></li><br>
                            <ul>
                                <li><strong>Singly Linked List:</strong> Create a new node, traverse to the node before the desired position, update the previous node's next pointer, set the new node's next pointer to the node that was previously in that position.</li><br>
                                <li><strong>Doubly Linked List:</strong> Create a new node, traverse to the node before the desired position, update the previous node's next pointer, update the new node's previous pointer, set the new node's next pointer, update the node after the new node's previous pointer.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Considerations:</strong></p><br>
                        <ul>
                            <li>Ensure that the correct pointers are updated during insertion to maintain the structure of the linked list.</li><br>
                            <li>Handle edge cases such as inserting into an empty list or at the beginning or end of the list.</li><br>
                        </ul><br>
                    </section><br>
                    
                    <section>
                        <h2>Deletion of Linked List</h2><br>
                        <p><strong>Types of Deletion:</strong></p><br>
                        <ul>
                            <li><strong>From the Beginning:</strong></li><br>
                            <ul>
                                <li><strong>Singly Linked List:</strong> Check if the list is not empty, update the head to point to the next node.</li><br>
                                <li><strong>Doubly Linked List:</strong> Check if the list is not empty, update the head to point to the next node, if the next node exists, set its previous pointer to `null`.</li><br>
                            </ul><br>
                            <li><strong>From the End:</strong></li><br>
                            <ul>
                                <li><strong>Singly Linked List:</strong> Traverse to the second-to-last node, update the last node's next pointer to `null`.</li><br>
                                <li><strong>Doubly Linked List:</strong> Update the tail to point to the previous node, update the tail's next pointer to `null`.</li><br>
                            </ul><br>
                            <li><strong>From a Specific Position:</strong></li><br>
                            <ul>
                                <li><strong>Singly Linked List:</strong> Traverse to the node before the target node, update the previous node's next pointer to point to the target node's next node.</li><br>
                                <li><strong>Doubly Linked List:</strong> Traverse to the target node, update the previous node's next pointer, update the next node's previous pointer.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Considerations:</strong></p><br>
                        <ul>
                            <li>Error Handling: Ensure the list is not empty before attempting deletion.</li><br>
                            <li>Edge Cases: Handle deletion when the list is empty, at the beginning, or at the end.</li><br>
                            <li>Memory Management: Properly free the memory allocated to the deleted node to avoid memory leaks.</li><br>
                        </ul><br>
                    </section>

                    <section>
                        <h2>Concatenation of Linked Lists</h2><br>
                        <p><strong>Concatenation in Singly Linked Lists:</strong></p><br>
                        <ul>
                            <li><strong>Steps:</strong></li><br>
                            <ul>
                                <li>If either list is empty, return the other list as the result.</li><br>
                                <li>Traverse to the end of the first list (the last node).</li><br>
                                <li>Set the last node's next pointer to the head of the second list.</li><br>
                            </ul><br>
                            <li><strong>Time Complexity:</strong></li><br>
                            <ul>
                                <li>Traversing to the end of the first list requires O(n), where n is the length of the first list.</li><br>
                                <li>The operation itself is completed in constant time once the end of the first list is reached.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Concatenation in Doubly Linked Lists:</strong></p><br>
                        <ul>
                            <li><strong>Steps:</strong></li><br>
                            <ul>
                                <li>If either list is empty, return the other list as the result.</li><br>
                                <li>Set the tail of the first list's next pointer to the head of the second list.</li><br>
                                <li>Set the head of the second list's previous pointer to the tail of the first list.</li><br>
                                <li>Update the tail of the concatenated list to be the tail of the second list.</li><br>
                            </ul><br>
                            <li><strong>Time Complexity:</strong></li><br>
                            <ul>
                                <li>Since the tail of the first list is immediately accessible, the concatenation is performed in constant time, O(1).</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Considerations:</strong></p><br>
                        <ul>
                            <li><strong>Edge Cases:</strong> If either list is empty, the concatenation should handle this gracefully.</li><br>
                            <li><strong>Memory Management:</strong> Ensure that the resulting list is properly maintained, particularly when the operation affects other references to the individual lists.</li><br>
                        </ul><br>
                    </section><br>
                    
                    <section>
                        <h2>Traversal of Linked Lists</h2><br>
                        <p><strong>Traversal in Singly Linked Lists:</strong></p><br>
                        <ul>
                            <li><strong>Steps:</strong></li><br>
                            <ul>
                                <li>Start at the head of the list.</li><br>
                                <li>Use a loop to iterate through the list.</li><br>
                                <li>Access or manipulate the data at the current node.</li><br>
                                <li>Move to the next node by following the next pointer.</li><br>
                                <li>Continue until the end of the list (when the next pointer is `null`).</li><br>
                            </ul><br>
                            <li><strong>Use Cases:</strong></li><br>
                            <ul>
                                <li>Searching for an element in the list.</li><br>
                                <li>Performing operations on each element (e.g., printing or modifying data).</li><br>
                                <li>Calculating the length of the list.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Traversal in Doubly Linked Lists:</strong></p><br>
                        <ul>
                            <li><strong>Forward Traversal:</strong></li><br>
                            <ul>
                                <li>Start at the head of the list.</li><br>
                                <li>Use a loop to iterate through the list.</li><br>
                                <li>Access or manipulate the data at the current node.</li><br>
                                <li>Move to the next node by following the next pointer.</li><br>
                                <li>Continue until the end of the list (when the next pointer is `null`).</li><br>
                            </ul><br>
                            <li><strong>Backward Traversal:</strong></li><br>
                            <ul>
                                <li>Start at the tail of the list.</li><br>
                                <li>Use a loop to iterate backward through the list.</li><br>
                                <li>Access or manipulate the data at the current node.</li><br>
                                <li>Move to the previous node by following the previous pointer.</li><br>
                                <li>Continue until the beginning of the list (when the previous pointer is `null`).</li><br>
                            </ul><br>
                            <li><strong>Use Cases:</strong></li><br>
                            <ul>
                                <li>Forward traversal is similar to that in singly linked lists.</li><br>
                                <li>Backward traversal can be useful for accessing elements in reverse order.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Considerations:</strong></p><br>
                        <ul>
                            <li><strong>Edge Cases:</strong> Ensure that the list is not empty before attempting traversal.</li><br>
                            <li><strong>Termination:</strong> Properly handle loop termination to avoid infinite loops during traversal.</li><br>
                        </ul><br>
                    </section>

                    <section>
                        <h2>Dynamic Memory Management</h2><br>
                        <p><strong>Dynamic memory management</strong> refers to the process of allocating and deallocating memory during runtime for data structures and variables, allowing data structures such as linked lists to grow or shrink based on the program's needs.</p><br>
                        <p><strong>Key Concepts:</strong></p><br>
                        <ul>
                            <li><strong>Heap Memory:</strong></li><br>
                            <ul>
                                <li>Most dynamic data structures use heap memory for allocation.</li><br>
                                <li>Memory is allocated as needed, allowing data structures to grow dynamically.</li><br>
                            </ul><br>
                            <li><strong>Memory Allocation:</strong></li><br>
                            <ul>
                                <li>Memory can be allocated for new nodes in linked lists or other data structures.</li><br>
                                <li>Allocation is often performed using functions such as <code>malloc</code> (C) or <code>new</code> (C++), depending on the programming language.</li><br>
                            </ul><br>
                            <li><strong>Memory Deallocation:</strong></li><br>
                            <ul>
                                <li>Properly freeing memory is crucial to avoid memory leaks and free up resources.</li><br>
                                <li>Deallocation is typically done using functions such as <code>free</code> (C) or <code>delete</code> (C++), depending on the programming language.</li><br>
                            </ul><br>
                            <li><strong>Garbage Collection:</strong></li><br>
                            <ul>
                                <li>In languages with automatic memory management (e.g., Java, Python), the runtime system handles memory allocation and deallocation automatically through garbage collection.</li><br>
                                <li>This helps prevent memory leaks but may introduce overhead.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Best Practices:</strong></p><br>
                        <ul>
                            <li><strong>Proper Memory Management:</strong> Allocate and deallocate memory as needed and avoid excessive allocation. Always free memory when it is no longer needed to prevent memory leaks.</li><br>
                            <li><strong>Monitoring Memory Usage:</strong> Track memory usage during development and testing to identify potential memory issues.</li><br>
                            <li><strong>Minimizing Fragmentation:</strong> Use memory allocation strategies that reduce fragmentation (scattered free blocks of memory).</li><br>
                        </ul><br>
                        <p><strong>Challenges:</strong></p><br>
                        <ul>
                            <li><strong>Memory Leaks:</strong> Occur when allocated memory is not deallocated, causing the program to consume more memory than necessary.</li><br>
                            <li><strong>Fragmentation:</strong> Memory fragmentation can occur if deallocation results in scattered free blocks of memory, potentially affecting performance.</li><br>
                        </ul><br>
                        <p>Dynamic memory management is essential for optimizing memory usage and performance in programs that work with dynamic data structures like linked lists. Proper memory management ensures efficient and reliable program execution.</p><br>
                    </section><br>
                    
                    <section>
                        <h2>Garbage Collection</h2><br>
                        <p><strong>Garbage Collection:</strong></p><br>
                        <ul>
                            <li>Garbage collection (GC) is the process of automatically reclaiming memory occupied by objects that are no longer in use by a program.</li><br>
                            <li>It is a feature commonly found in high-level programming languages like Java, Python, and C#.</li><br>
                        </ul><br>
                        <p><strong>How Garbage Collection Works:</strong></p><br>
                        <ul>
                            <li><strong>Memory Allocation:</strong></li><br>
                            <ul>
                                <li>When an object is created, memory is allocated on the heap.</li><br>
                            </ul><br>
                            <li><strong>Tracking References:</strong></li><br>
                            <ul>
                                <li>The garbage collector tracks object references to identify which objects are still in use.</li><br>
                            </ul><br>
                            <li><strong>Reclaiming Memory:</strong></li><br>
                            <ul>
                                <li>Objects without any references are considered "garbage."</li><br>
                                <li>The garbage collector periodically reclaims memory occupied by garbage to free up space.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Garbage Collection Algorithms:</strong></p><br>
                        <ul>
                            <li><strong>Reference Counting:</strong></li><br>
                            <ul>
                                <li>Tracks the number of references to an object.</li><br>
                                <li>When the reference count reaches zero, the object is eligible for garbage collection.</li><br>
                            </ul><br>
                            <li><strong>Mark-and-Sweep:</strong></li><br>
                            <ul>
                                <li>Marks all objects reachable from root references (e.g., global variables, stack).</li><br>
                                <li>Sweeps through memory, collecting unmarked objects as garbage.</li><br>
                            </ul><br>
                            <li><strong>Copying Collection:</strong></li><br>
                            <ul>
                                <li>Divides memory into regions (from-space and to-space).</li><br>
                                <li>Copies live objects from from-space to to-space, discarding the rest.</li><br>
                            </ul><br>
                            <li><strong>Generational Collection:</strong></li><br>
                            <ul>
                                <li>Objects are categorized by age into generations.</li><br>
                                <li>Younger generations are collected more frequently, as most objects have short lifetimes.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Advantages of Garbage Collection:</strong></p><br>
                        <ul>
                            <li><strong>Automatic Memory Management:</strong> Relieves developers from manually managing memory, reducing errors like memory leaks and dangling pointers.</li><br>
                            <li><strong>Improved Program Reliability:</strong> Helps maintain program stability and prevents crashes caused by memory management issues.</li><br>
                        </ul><br>
                        <p><strong>Disadvantages of Garbage Collection:</strong></p><br>
                        <ul>
                            <li><strong>Performance Overhead:</strong> Garbage collection can cause pauses in program execution, especially with stop-the-world collections.</li><br>
                            <li><strong>Complexity:</strong> The implementation of efficient garbage collection algorithms can be complex.</li><br>
                        </ul><br>
                        <p><strong>Best Practices:</strong></p><br>
                        <ul>
                            <li><strong>Minimize Unnecessary Object Creation:</strong> Reducing the number of short-lived objects can improve garbage collection performance.</li><br>
                            <li><strong>Be Aware of Object Lifetimes:</strong> Understanding how long objects should remain in memory can help write more efficient programs.</li><br>
                            <li><strong>Avoid Cyclic References:</strong> Minimize or carefully handle cyclic references to prevent memory leaks.</li><br>
                        </ul><br>
                        <p>Garbage collection is a useful tool for managing memory automatically, allowing developers to focus on writing code without worrying about manual memory management. However, its performance implications should be considered during development.</p><br>
                    </section>
                    
                </div>

                <div id="unit4" class="formatted-text">
                    <!-- Unit 4 text -->
                    <h2>UNIT 4: Trees and Graphs</h2>
                    <br>
                    <section>
                        <h2>Trees</h2><br>
                        <p><strong>Node:</strong> The basic unit of a tree, representing a single element or value.</p><br>
                        <p><strong>Edge:</strong> The connection between nodes, showing the parent-child relationship.</p><br>
                        <p><strong>Root:</strong> The topmost node of the tree with no parent.</p><br>
                        <p><strong>Leaf:</strong> Nodes with no children.</p><br>
                        <p><strong>Parent:</strong> A node that has one or more children nodes.</p><br>
                        <p><strong>Child:</strong> A node that is connected to a parent node.</p><br>
                        <p><strong>Siblings:</strong> Nodes that share the same parent.</p><br>
                        <p><strong>Height:</strong> The number of edges on the longest path from the root to a leaf.</p><br>
                        <p><strong>Depth/Level:</strong> The distance from the root node to a specific node (number of edges).</p><br>
                        <p><strong>Degree:</strong> The number of children a node has.</p><br>
                    </section><br>
                    
                    <section>
                        <h2>Graphs</h2><br>
                        <p><strong>Vertex:</strong> The basic unit of a graph, also known as a node.</p><br>
                        <p><strong>Edge:</strong> A connection between two vertices, representing a relationship.</p><br>
                        <p><strong>Directed Graph:</strong> A graph where edges have direction (e.g., from A to B).</p><br>
                        <p><strong>Undirected Graph:</strong> A graph where edges have no direction (e.g., between A and B).</p><br>
                        <p><strong>Weight:</strong> A value assigned to an edge in weighted graphs, representing cost or distance.</p><br>
                        <p><strong>Path:</strong> A sequence of vertices connected by edges.</p><br>
                        <p><strong>Cycle:</strong> A path that starts and ends at the same vertex.</p><br>
                        <p><strong>Adjacent:</strong> Two vertices connected by an edge.</p><br>
                        <p><strong>Degree:</strong> The number of edges incident to a vertex.</p><br>
                        <p><strong>Graph Types:</strong></p><br>
                        <ul>
                            <li><strong>Connected Graph:</strong> A graph in which there is a path from any vertex to every other vertex.</li><br>
                            <li><strong>Disconnected Graph:</strong> A graph consisting of more than one component.</li><br>
                            <li><strong>Bipartite Graph:</strong> A graph where vertices can be divided into two disjoint sets such that no two vertices within the same set are adjacent.</li><br>
                        </ul><br>
                    </section>

                    <section>
                        <h2>Binary Trees and Its Representation: Insertion and Deletion of Nodes in Binary Tree</h2><br>
                        <h3>Binary Trees:</h3><br>
                        <p>A binary tree is a tree data structure where each node has at most two children, referred to as the left child and right child.</p><br>
                        
                        <h3>Representation:</h3><br>
                        <ul>
                            <li><strong>Array Representation:</strong></li><br>
                            <ul>
                                <li>Binary trees can be represented using arrays, where the root is at index 0, and for a node at index <code>i</code>, its left child is at <code>2i + 1</code> and its right child is at <code>2i + 2</code>.</li><br>
                            </ul>
                            <li><strong>Linked List Representation:</strong></li><br>
                            <ul>
                                <li>Each node contains data and references (pointers) to its left and right children. This is the most common representation for binary trees.</li><br>
                            </ul>
                        </ul><br>
                        
                        <h3>Insertion in Binary Trees:</h3><br>
                        <p><strong>Steps:</strong></p><br>
                        <ol>
                            <li>Find the location to insert the new node.</li><br>
                            <li>If the tree is empty, set the new node as the root.</li><br>
                            <li>Otherwise, use a level-order traversal (breadth-first search) to find the first available spot (a node with fewer than two children).</li><br>
                            <li>Insert the new node as a child of the found node.</li><br>
                        </ol><br>
                        <p><strong>Time Complexity:</strong> Insertion typically takes O(n) time, where <code>n</code> is the number of nodes, as it may require traversing the entire tree in the worst case.</p><br>
                        
                        <h3>Deletion in Binary Trees:</h3><br>
                        <p><strong>Steps:</strong></p><br>
                        <ol>
                            <li>Find the node to delete and the last node in the tree.</li><br>
                            <li>Replace the node to delete with the last node in the tree.</li><br>
                            <li>Remove the last node.</li><br>
                        </ol><br>
                        <p><strong>Time Complexity:</strong> Deletion typically takes O(n) time, where <code>n</code> is the number of nodes, as it may require traversing the entire tree in the worst case.</p><br>
                        
                        <h3>Considerations:</h3><br>
                        <ul>
                            <li><strong>Balancing:</strong> Keeping a binary tree balanced can help optimize operations such as insertion, deletion, and search.</li><br>
                            <li><strong>Edge Cases:</strong> Properly handle cases such as deleting a node with one child, no children, or two children.</li><br>
                        </ul><br>
                        
                        <p>Insertion and deletion in binary trees can be managed using various techniques to optimize performance, depending on whether the binary tree is a regular binary tree or a specialized type such as a binary search tree.</p><br>
                    </section>

                    <section>
                        <h2>Binary Search Tree (BST)</h2><br>
                        <p>A binary search tree (BST) is a binary tree where each node follows the order properties:</p><br>
                        <ul>
                            <li>The left child's key is less than the parent node's key.</li><br>
                            <li>The right child's key is greater than the parent node's key.</li><br>
                        </ul><br>
                    </section>
                    
                    <section>
                        <h2>Traversal in Binary Search Tree</h2><br>
                        <p><strong>Inorder Traversal:</strong></p><br>
                        <ul>
                            <li>Visit the left subtree.</li><br>
                            <li>Visit the root node (current node).</li><br>
                            <li>Visit the right subtree.</li><br>
                            <li>This traversal produces a sorted sequence of the node keys.</li><br>
                        </ul><br>
                        <p><strong>Preorder Traversal:</strong></p><br>
                        <ul>
                            <li>Visit the root node (current node).</li><br>
                            <li>Visit the left subtree.</li><br>
                            <li>Visit the right subtree.</li><br>
                            <li>Preorder traversal can be useful for creating a copy of the tree.</li><br>
                        </ul><br>
                        <p><strong>Postorder Traversal:</strong></p><br>
                        <ul>
                            <li>Visit the left subtree.</li><br>
                            <li>Visit the right subtree.</li><br>
                            <li>Visit the root node (current node).</li><br>
                            <li>Postorder traversal can be useful for deleting the tree.</li><br>
                        </ul><br>
                    </section>
                    
                    <section>
                        <h2>Traversal Implementation</h2><br>
                        <p><strong>Recursive Approach:</strong></p><br>
                        <ul>
                            <li>Implement each traversal method using recursion.</li><br>
                            <li>For each type of traversal, call the traversal function recursively for the left subtree, root, and right subtree, in the correct order.</li><br>
                        </ul><br>
                        <p><strong>Iterative Approach:</strong></p><br>
                        <ul>
                            <li>Implement each traversal method using a stack to mimic recursion.</li><br>
                            <li>Perform the traversal using a loop and the stack to hold the nodes for later processing.</li><br>
                        </ul><br>
                    </section>
                    
                    <section>
                        <h2>Benefits of Traversal</h2><br>
                        <p><strong>Inorder Traversal:</strong></p><br>
                        <ul>
                            <li>Inorder traversal is particularly important in binary search trees because it produces a sorted sequence of keys.</li><br>
                        </ul><br>
                        <p><strong>Preorder and Postorder Traversal:</strong></p><br>
                        <ul>
                            <li>Preorder and postorder traversals provide insights into the tree structure and are useful for certain tree operations.</li><br>
                        </ul><br>
                    </section>

                    <section>
                        <h2>Threaded Binary Tree</h2><br>
                        <p>A threaded binary tree is a binary tree in which additional pointers (threads) are used to make in-order traversal easier and faster.</p><br>
                        <p><strong>Types of Threaded Binary Trees:</strong></p><br>
                        <ul>
                            <li><strong>Single Threaded:</strong> Only the right (or left) pointer of each node may be a thread pointing to the in-order successor (or predecessor).</li><br>
                            <li><strong>Double Threaded:</strong> Both the left and right pointers may be threads pointing to the in-order predecessor and successor, respectively.</li><br>
                        </ul><br>
                        <p><strong>Purpose:</strong></p><br>
                        <ul>
                            <li>Threaded binary trees optimize in-order traversal by avoiding the overhead of recursion and stacks.</li><br>
                            <li>They maintain all the features of binary trees while allowing efficient traversal.</li><br>
                        </ul><br>
                        <p><strong>Insertion in Threaded Binary Trees:</strong></p><br>
                        <p><strong>Steps:</strong></p><br>
                        <ul>
                            <li>Insert the node as in a regular binary tree, ensuring it follows BST properties.</li><br>
                            <li>Update the threads as necessary to maintain in-order traversal links.</li><br>
                        </ul><br>
                        <p><strong>Deletion in Threaded Binary Trees:</strong></p><br>
                        <p><strong>Steps:</strong></p><br>
                        <ul>
                            <li>Delete the node as in a regular binary tree.</li><br>
                            <li>Reconnect threads of the node's predecessors and successors as necessary to maintain proper in-order traversal links.</li><br>
                        </ul><br>
                        <p><strong>Traversal in Threaded Binary Trees:</strong></p><br>
                        <p><strong>In-order Traversal:</strong></p><br>
                        <ul>
                            <li>Start from the leftmost node.</li><br>
                            <li>Follow the in-order successor threads to traverse the tree in order.</li><br>
                        </ul><br>
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>Efficient in-order traversal: Faster traversal since there is no need for recursion or a stack.</li><br>
                            <li>Space-efficient: No additional data structures are required for traversal.</li><br>
                        </ul><br>
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>Complexity: Managing threads during insertions and deletions can be more complex than standard binary trees.</li><br>
                            <li>Limited use: Threaded binary trees are typically used when in-order traversal is a priority.</li><br>
                        </ul><br>
                    </section>

                    <section>
                        <h2>Heap</h2><br>
                        <p><strong>Definition:</strong></p><br>
                        <ul>
                            <li>A heap is a specialized tree-based data structure that satisfies the heap property.</li><br>
                            <li>It can be a max heap or min heap.</li><br>
                        </ul><br>
                        
                        <p><strong>Heap Properties:</strong></p><br>
                        <ul>
                            <li><strong>Max Heap:</strong></li>
                            <ul>
                                <li>The key (value) at the root node is greater than or equal to the keys of its children.</li><br>
                                <li>This property is recursively true for all nodes in the tree.</li><br>
                            </ul><br>
                            <li><strong>Min Heap:</strong></li>
                            <ul>
                                <li>The key at the root node is less than or equal to the keys of its children.</li><br>
                                <li>This property is recursively true for all nodes in the tree.</li><br>
                            </ul><br>
                        </ul><br>
                        
                        <p><strong>Representation:</strong></p><br>
                        <ul>
                            <li>Heaps are typically represented using arrays.</li><br>
                            <li>The root is stored at index 0, and for a node at index <code>i</code>, its left child is at <code>2i + 1</code> and its right child is at <code>2i + 2</code>.</li><br>
                        </ul><br>
                        
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Insertion:</strong></li>
                            <ul>
                                <li>Insert the new element at the end of the heap (array).</li><br>
                                <li>Heapify up (bubble up) to maintain the heap property by comparing the new element with its parent and swapping if necessary.</li><br>
                            </ul><br>
                            <li><strong>Deletion:</strong></li>
                            <ul>
                                <li>Remove the root element (maximum or minimum element).</li><br>
                                <li>Replace the root with the last element in the heap.</li><br>
                                <li>Heapify down (bubble down) to maintain the heap property by comparing the root with its children and swapping if necessary.</li><br>
                            </ul><br>
                            <li><strong>Peek:</strong></li>
                            <ul>
                                <li>Return the root element (maximum or minimum element) without removing it.</li><br>
                            </ul><br>
                            <li><strong>Heapify:</strong></li>
                            <ul>
                                <li>The process of converting a binary tree (array) into a heap.</li><br>
                                <li>Can be done in O(n) time complexity.</li><br>
                            </ul><br>
                        </ul><br>
                        
                        <p><strong>Applications:</strong></p><br>
                        <ul>
                            <li><strong>Priority Queues:</strong></li>
                            <ul>
                                <li>Heaps are often used to implement priority queues for efficient insertion and deletion.</li><br>
                            </ul><br>
                            <li><strong>Heap Sort:</strong></li>
                            <ul>
                                <li>Heaps can be used in sorting algorithms for efficient sorting (O(n log n) time complexity).</li><br>
                            </ul><br>
                            <li><strong>Graph Algorithms:</strong></li>
                            <ul>
                                <li>Heaps are used in algorithms like Dijkstra's algorithm for efficient shortest path calculations.</li><br>
                            </ul><br>
                        </ul><br>
                        
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li><strong>Efficient Operations:</strong></li>
                            <ul>
                                <li>Insertions and deletions can be performed in O(log n) time.</li><br>
                                <li>Accessing the root element (maximum or minimum) is constant time, O(1).</li><br>
                            </ul><br>
                            <li><strong>Space Efficiency:</strong></li>
                            <ul>
                                <li>Heaps are compactly stored in arrays, using a contiguous block of memory.</li><br>
                            </ul><br>
                        </ul><br>
                        
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li><strong>Complexity:</strong></li>
                            <ul>
                                <li>The heap property must be maintained during insertions and deletions, which can add complexity to operations.</li><br>
                            </ul><br>
                        </ul><br>
                        
                        <p>Heaps are versatile data structures that provide efficient support for priority queues, sorting, and graph algorithms, among other applications.</p><br>
                    </section>

                    <section>
                        <h2>Balanced Trees</h2><br>
                        <p><strong>Definition:</strong></p><br>
                        <p>Balanced trees are types of trees where the height is kept as low as possible, ensuring efficient operations such as insertion, deletion, and searching.</p><br>
                        
                        <p><strong>Types of Balanced Trees:</strong></p><br>
                        <ul>
                            <li><strong>AVL Tree:</strong></li>
                            <ul>
                                <li>AVL stands for Adelson-Velsky and Landis, the inventors of the tree.</li><br>
                                <li>Each node maintains a balance factor, which is the height difference between the left and right subtrees.</li><br>
                                <li>When the balance factor is not within the acceptable range (-1, 0, 1), rotations are performed to rebalance the tree.</li><br>
                            </ul><br>
                            <li><strong>Red-Black Tree:</strong></li>
                            <ul>
                                <li>A red-black tree is a type of self-balancing binary search tree.</li><br>
                                <li>Each node is colored red or black, and the tree follows specific properties related to node colors and paths.</li><br>
                                <li>Violations of the red-black tree properties trigger rotations and color changes to restore balance.</li><br>
                            </ul><br>
                            <li><strong>Splay Tree:</strong></li>
                            <ul>
                                <li>A splay tree is a self-adjusting binary search tree.</li><br>
                                <li>Operations like insertion, deletion, and search involve "splaying" (moving) the accessed node to the root to maintain balance.</li><br>
                            </ul><br>
                            <li><strong>B-Tree:</strong></li>
                            <ul>
                                <li>B-Trees are generalizations of binary search trees where nodes can have more than two children.</li><br>
                                <li>Commonly used in databases and filesystems.</li><br>
                                <li>Ensures that all leaf nodes are at the same level and maintains a minimum and maximum number of children for each node.</li><br>
                            </ul><br>
                        </ul><br>
                        
                        <p><strong>Operations:</strong></p><br>
                        <ul>
                            <li><strong>Insertion and Deletion:</strong></li>
                            <ul>
                                <li>Insertion and deletion operations in balanced trees are similar to those in binary search trees but may trigger rebalancing operations like rotations or splits.</li><br>
                            </ul><br>
                            <li><strong>Searching:</strong></li>
                            <ul>
                                <li>Searching is efficient in balanced trees because the height is minimized, resulting in shorter paths from the root to any node.</li><br>
                            </ul><br>
                        </ul><br>
                        
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li><strong>Efficient Operations:</strong></li>
                            <ul>
                                <li>Maintaining balance ensures efficient operations, with O(log n) time complexity for insertion, deletion, and searching.</li><br>
                            </ul><br>
                            <li><strong>Consistency:</strong></li>
                            <ul>
                                <li>Balanced trees provide consistent performance across operations.</li><br>
                            </ul><br>
                        </ul><br>
                        
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li><strong>Complex Implementation:</strong></li>
                            <ul>
                                <li>Maintaining balance requires additional logic such as rotations, which can complicate implementation.</li><br>
                            </ul><br>
                            <li><strong>Performance Overhead:</strong></li>
                            <ul>
                                <li>Rebalancing operations can add overhead to insertion and deletion.</li><br>
                            </ul><br>
                        </ul><br>
                        
                        <p>Balanced trees are widely used in applications where performance and efficiency are important, such as databases, filesystems, and data processing systems.</p><br>
                    </section>

                    <section>
                        <h2>Terminology and representation of graphs using adjacency matrix:</h2><br>
                        <p><strong>Graph Terminology:</strong></p><br>
                        <ul>
                            <li><strong>Vertex (Node):</strong> A fundamental unit in a graph, representing an object or an entity.</li><br>
                            <li><strong>Edge:</strong> A connection between two vertices, representing a relationship.</li><br>
                            <li><strong>Directed Graph (Digraph):</strong> A graph where edges have direction, going from one vertex to another.</li><br>
                            <li><strong>Undirected Graph:</strong> A graph where edges have no direction, meaning the relationship is bidirectional.</li><br>
                            <li><strong>Weight:</strong> The cost or value assigned to an edge in a weighted graph.</li><br>
                            <li><strong>Path:</strong> A sequence of vertices connected by edges.</li><br>
                            <li><strong>Cycle:</strong> A path that starts and ends at the same vertex.</li><br>
                            <li><strong>Adjacent:</strong> Two vertices connected by an edge.</li><br>
                        </ul><br>
                        
                        <p><strong>Adjacency Matrix Representation:</strong></p><br>
                        <ul>
                            <li><strong>Definition:</strong> An adjacency matrix is a 2D array used to represent a graph.</li><br>
                            <li><strong>Matrix Values:</strong> For an undirected graph, the matrix is symmetric.</li><br>
                            <li><strong>Benefits:</strong> Constant time complexity, O(1), to check if an edge exists between two vertices.</li><br>
                            <li><strong>Drawbacks:</strong> Space complexity is O(n^2), which can be inefficient for sparse graphs.</li><br>
                        </ul><br>
                        
                        <p><strong>Applications:</strong> Adjacency matrix representation is useful in applications where dense graphs are used, and fast access to edges is needed.</p><br>
                        
                        <h2>Warshall's algorithm:</h2><br>
                        <p><strong>Warshall's Algorithm:</strong></p><br>
                        <ul>
                            <li><strong>Key Concepts:</strong> Transitive Closure</li><br>
                            <li><strong>Algorithm Steps:</strong> Initialization, Warshall's Algorithm</li><br>
                            <li><strong>Time Complexity:</strong> O(n^3)</li><br>
                            <li><strong>Applications:</strong> Graph Reachability, Transitive Closure</li><br>
                            <li><strong>Advantages:</strong> Simple to implement and understand</li><br>
                            <li><strong>Disadvantages:</strong> Less efficient for large graphs due to the O(n^3) time complexity</li><br>
                        </ul><br>
                        
                        <p>Warshall's algorithm is useful for finding the transitive closure of a graph, providing insights into the reachability of vertices and potential paths between them.</p><br>
                    </section>
                    
                </div>

                <div id="unit5" class="formatted-text">
                    <!-- Unit 5 text -->
                    <h2>UNIT 5: Searching and Sorting</h2>
                    <br>
                    <section>
                        <h2>Sequential Searching:</h2><br>
                        <p><strong>Definition:</strong> Sequential searching, also known as linear searching, is a search algorithm that checks each element in a list sequentially from the beginning until the desired element is found.</p><br>
                        <p><strong>How Sequential Searching Works:</strong></p><br>
                        <ul>
                            <li><strong>Time Complexity:</strong> Best Case: O(1), Average Case: O(n/2), Worst Case: O(n)</li><br>
                            <li><strong>Space Complexity:</strong> O(1)</li><br>
                        </ul><br>
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>Simple and easy to understand.</li><br>
                            <li>Suitable for small lists where search speed is less critical.</li><br>
                        </ul><br>
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>Inefficient for large lists due to the linear time complexity.</li><br>
                            <li>Not as efficient as other search algorithms for larger datasets.</li><br>
                        </ul><br>
                        <p><strong>Applications:</strong></p><br>
                        <p>Sequential searching is a straightforward search method that works well for small or unsorted lists, but its linear time complexity makes it less efficient for larger datasets.</p><br>
                    </section>
                    
                    <section>
                        <h2>Binary searching:</h2><br>
                        <p><strong>Definition:</strong> Binary searching is an efficient search algorithm that finds the position of a target value within a sorted array.</p><br>
                        <p><strong>How Binary Searching Works:</strong></p><br>
                        <ul>
                            <li><strong>Time Complexity:</strong> O(log n)</li><br>
                            <li><strong>Space Complexity:</strong> O(1)</li><br>
                        </ul><br>
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>Fast and efficient for large sorted lists.</li><br>
                            <li>Works well with large datasets when the list is sorted.</li><br>
                        </ul><br>
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>Requires the list to be sorted for the algorithm to work correctly.</li><br>
                            <li>Can only be used with arrays or other data structures that allow random access.</li><br>
                        </ul><br>
                        <p><strong>Applications:</strong></p><br>
                        <p>Binary searching provides an efficient way to search for a target value in a sorted array, making it a valuable tool for handling large data sets in applications that require fast search operations.</p><br>
                    </section>

                    <section>
                        <h2>Skip Lists</h2><br>
                        <p>- Skip lists are a probabilistic data structure that allows for fast search, insertion, and deletion operations in an ordered list.</p><br>
                        <p>- They are similar to linked lists but add multiple levels of linked lists to allow for quicker traversal.</p><br>
                    </section>
                    
                    <section>
                        <h2>Dictionaries</h2><br>
                        <p>- A dictionary is a data structure that stores key-value pairs, providing efficient access, insertion, and deletion based on keys.</p><br>
                        <p>- Skip lists can be used to implement dictionaries by using keys as the ordered elements in the list.</p><br>
                    </section>
                    
                    <section>
                        <h2>Linear List Representation</h2><br>
                        <p>- In a linear list, data elements are stored in a single list, typically a linked list or an array.</p><br>
                        <p>- Search operations in a linear list have linear time complexity, O(n), as each element must be checked sequentially.</p><br>
                        <p>- Skip lists provide a more efficient alternative to linear lists for ordered data.</p><br>
                    </section>
                    
                    <section>
                        <h2>Skip List Representation</h2><br>
                        <p><strong>Levels:</strong></p><br>
                        <ul>
                            <li>- Skip lists consist of multiple levels of linked lists.</li><br>
                            <li>- The bottom level contains all elements in order, similar to a linked list.</li><br>
                            <li>- Higher levels contain a subset of the elements, allowing for faster traversal.</li><br>
                        </ul><br>
                        
                        <p><strong>Probability:</strong></p><br>
                        <ul>
                            <li>- Each element is included in a higher level list based on a probability (commonly 1/2).</li><br>
                        </ul><br>
                        
                        <p><strong>Search:</strong></p><br>
                        <ul>
                            <li>- Begin the search at the highest level and traverse forward, moving down a level if the next element is greater than the target.</li><br>
                            <li>- Continue the search until the target element is found or the search range is exhausted.</li><br>
                        </ul><br>
                        
                        <p><strong>Insertion:</strong></p><br>
                        <ul>
                            <li>- Insert the new element into the bottom level list.</li><br>
                            <li>- Randomly decide whether to insert the element into higher levels based on the probability.</li><br>
                            <li>- Update pointers at each level to maintain the skip list structure.</li><br>
                        </ul><br>
                        
                        <p><strong>Deletion:</strong></p><br>
                        <ul>
                            <li>- Search for the element to delete in the same manner as a regular search.</li><br>
                            <li>- Remove the element from each level's list.</li><br>
                            <li>- Update pointers as necessary to maintain the structure.</li><br>
                        </ul><br>
                        
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>- Efficient Search: Average search time is O(log n) due to the probabilistic hierarchy of levels.</li><br>
                            <li>- Fast Insertions and Deletions: Insertions and deletions can be performed in O(log n) time on average.</li><br>
                            <li>- Dynamic Structure: Skip lists can grow and shrink dynamically as elements are added or removed.</li><br>
                        </ul><br>
                        
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>- Memory Overhead: Additional levels require more memory for pointers.</li><br>
                            <li>- Probabilistic Nature: Performance can vary due to the random nature of level assignments.</li><br>
                        </ul><br>
                        
                        <p>"Skip lists: Dictionaries, Linear list representation, Skip list representation."</p><br>
                    </section>

                    <section>
                        <h2>Operations with Skip Lists: Insertion, Deletion, Searching</h2><br>
                        
                        <h3>Insertion:</h3><br>
                        <p><strong>Steps:</strong></p><br>
                        <ul>
                            <li>- Find the position:</li><br>
                            <ul>
                                <li>- Start the search at the highest level and traverse forward until reaching a position where the next element is greater than the element to be inserted.</li><br>
                                <li>- Move down a level and repeat until the bottom level is reached.</li><br>
                            </ul><br>
                            <li>- Insert the new element:</li><br>
                            <ul>
                                <li>- At the bottom level, insert the new element.</li><br>
                                <li>- Randomly decide which higher levels to insert the new element into, based on a probability (commonly 1/2).</li><br>
                                <li>- Update pointers at each level where the new element is inserted to maintain the skip list structure.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Time Complexity:</strong> Average case is O(log n), where `n` is the number of elements in the list.</p><br>
                        <p><strong>Considerations:</strong> Handle edge cases where the list is empty or the new element is the smallest or largest element.</p><br>
                        
                        <h3>Deletion:</h3><br>
                        <p><strong>Steps:</strong></p><br>
                        <ul>
                            <li>- Find the element to delete:</li><br>
                            <ul>
                                <li>- Start the search at the highest level and traverse forward until reaching a position where the next element is equal to the element to be deleted.</li><br>
                                <li>- Move down a level and repeat until the bottom level is reached.</li><br>
                            </ul><br>
                            <li>- Remove the element:</li><br>
                            <ul>
                                <li>- At each level, update the pointers to bypass the node being removed.</li><br>
                                <li>- Continue until the element is removed from all levels where it exists.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Time Complexity:</strong> Average case is O(log n).</p><br>
                        <p><strong>Considerations:</strong> Properly update pointers at each level to maintain the skip list structure. Handle edge cases where the element to be deleted does not exist.</p><br>
                        
                        <h3>Searching:</h3><br>
                        <p><strong>Steps:</strong></p><br>
                        <ul>
                            <li>- Start at the top:</li><br>
                            <ul>
                                <li>- Begin the search at the highest level and traverse forward until reaching a position where the next element is greater than the target element.</li><br>
                                <li>- If the target element is found, return its position.</li><br>
                            </ul><br>
                            <li>- Move down a level:</li><br>
                            <ul>
                                <li>- If not found, move down a level and continue searching.</li><br>
                                <li>- Repeat until the bottom level is reached.</li><br>
                            </ul><br>
                        </ul><br>
                        <p><strong>Time Complexity:</strong> Average case is O(log n), providing fast search times.</p><br>
                        <p><strong>Considerations:</strong> Efficiently handle edge cases such as reaching the end of the list without finding the target element.</p><br>
                        
                        <p>Skip lists provide efficient operations for insertion, deletion, and searching, offering a good balance between complexity and performance. Their probabilistic nature allows for fast operations on average, making them a useful data structure for managing ordered data.</p><br>
                    </section>
                    
                    <section>
                        <h2>Insertion Sort</h2><br>
                        <p><strong>Definition:</strong> Insertion sort is a simple sorting algorithm that builds a sorted portion of the list one element at a time.</p><br>
                        <p><strong>How it Works:</strong></p><br>
                        <ul>
                            <li>Iterate through each element of the list.</li><br>
                            <li>For each element, compare it with the elements in the sorted portion and insert it at the correct position.</li><br>
                            <li>Shift larger elements to the right to make space for the new element.</li><br>
                        </ul><br>
                        <p><strong>Time Complexity:</strong></p><br>
                        <ul>
                            <li>Best Case: O(n) when the list is already sorted.</li><br>
                            <li>Average and Worst Case: O(n^2) due to nested loops for insertion.</li><br>
                        </ul><br>
                        <p><strong>Space Complexity:</strong> O(1), as sorting is done in place.</p><br>
                        <p><strong>Advantages:</strong></p><br>
                        <ul>
                            <li>Simple to implement.</li><br>
                            <li>Efficient for small or mostly sorted lists.</li><br>
                        </ul><br>
                        <p><strong>Disadvantages:</strong></p><br>
                        <ul>
                            <li>Inefficient for large or unsorted lists due to quadratic time complexity.</li><br>
                        </ul><br>
                    </section>
                    
                    <section>
                        <h2>Selection Sort</h2><br>
                        <p><strong>Definition:</strong> Selection sort is a simple sorting algorithm that divides the list into sorted and unsorted portions and repeatedly selects the minimum (or maximum) element from the unsorted portion and swaps it with the first element in the unsorted portion.</p><br>
                        <p><strong>How it Works:</strong></p><br>
                        <ul>
                            <li>Iterate through each element of the list.</li><br>
                            <li>For each element, find the minimum (or maximum) element in the remaining unsorted portion.</li><br>
                            <li>Swap the current element with the minimum (or maximum) element found.</li><br>
                        </ul><br>
                        <p><strong>Time Complexity:</strong> O(n^2), as it requires multiple passes to sort the list.</p><br>
                        <p><strong>Space Complexity:</strong> O(1), as sorting is done in place.</p><br>
                        <p><strong>Advantages:</strong> Simple to implement.</p><br>
                        <p><strong>Disadvantages:</strong> Inefficient for large lists due to quadratic time complexity.</p><br>
                    </section>
                    
                    <section>
                        <h2>Radix Sort</h2><br>
                        <p><strong>Definition:</strong> Radix sort is a non-comparative sorting algorithm that sorts elements based on their individual digits or values in specific bases (e.g., decimal, binary).</p><br>
                        <p><strong>How it Works:</strong></p><br>
                        <ul>
                            <li>Sort the elements based on the least significant digit (LSD).</li><br>
                            <li>Move on to the next significant digit and repeat the sorting process.</li><br>
                            <li>Continue until the most significant digit (MSD) has been processed.</li><br>
                        </ul><br>
                        <p><strong>Time Complexity:</strong> Average and Worst Case: O(d * n), where `d` is the number of digits and `n` is the number of elements.</p><br>
                        <p><strong>Space Complexity:</strong> O(n + k), where `k` is the number of buckets used in the sort (usually the base of the number system).</p><br>
                        <p><strong>Advantages:</strong> Can be very efficient for sorting numbers with a fixed number of digits.</p><br>
                        <p><strong>Disadvantages:</strong> Implementation complexity due to handling multiple buckets and digit positions.</p><br>
                    </section>

                    <section>
                        <h2>File Handling</h2><br>
                        <p>File handling refers to the management of data storage, retrieval, and modification in files on a storage device, such as a hard drive or solid-state drive.</p><br>
                    </section>
                    
                    <section>
                        <h2>File Operations</h2><br>
                        <p><strong>Open:</strong></p><br>
                        <ul>
                            <li>Opens a file for reading, writing, or appending.</li><br>
                            <li>Returns a file handle or file object, which is used to perform other operations on the file.</li><br>
                        </ul><br>
                        <p><strong>Close:</strong></p><br>
                        <ul>
                            <li>Closes a file, releasing resources associated with it.</li><br>
                        </ul><br>
                        <p><strong>Read:</strong></p><br>
                        <ul>
                            <li>Reads data from an open file.</li><br>
                            <li>Different read operations include reading the entire file, reading line by line, or reading a specific amount of data.</li><br>
                        </ul><br>
                        <p><strong>Write:</strong></p><br>
                        <ul>
                            <li>Writes data to an open file.</li><br>
                            <li>Can write data as text or binary, depending on the file mode.</li><br>
                        </ul><br>
                        <p><strong>Seek:</strong></p><br>
                        <ul>
                            <li>Moves the file pointer to a specified location within the file.</li><br>
                            <li>Useful for reading or writing data at specific positions.</li><br>
                        </ul><br>
                        <p><strong>Tell:</strong></p><br>
                        <ul>
                            <li>Returns the current position of the file pointer.</li><br>
                        </ul><br>
                        <p><strong>Append:</strong></p><br>
                        <ul>
                            <li>Adds data to the end of a file without modifying existing content.</li><br>
                        </ul><br>
                    </section>
                    
                    <section>
                        <h2>File Modes</h2><br>
                        <p>Different file modes specify the operations allowed on a file:</p><br>
                        <ul>
                            <li>Read (<code>r</code>): Opens a file for reading only.</li><br>
                            <li>Write (<code>w</code>): Opens a file for writing, truncating the file if it already exists.</li><br>
                            <li>Append (<code>a</code>): Opens a file for appending data to the end.</li><br>
                            <li>Read and Write (<code>r+</code>): Opens a file for reading and writing.</li><br>
                            <li>Binary Modes (<code>rb</code>, <code>wb</code>, <code>ab</code>): Binary versions of the above modes.</li><br>
                        </ul><br>
                    </section>
                    
                    <section>
                        <h2>File Handling in Programming Languages</h2><br>
                        <p><strong>Python:</strong></p><br>
                        <ul>
                            <li>Provides built-in functions (<code>open</code>, <code>close</code>, <code>read</code>, <code>write</code>, <code>seek</code>, <code>tell</code>) for file handling.</li><br>
                        </ul><br>
                        <p><strong>Java:</strong></p><br>
                        <ul>
                            <li>Offers classes like <code>FileReader</code>, <code>FileWriter</code>, and <code>RandomAccessFile</code> for file handling.</li><br>
                        </ul><br>
                        <p><strong>C and C++:</strong></p><br>
                        <ul>
                            <li>Use functions such as <code>fopen</code>, <code>fclose</code>, <code>fread</code>, <code>fwrite</code>, <code>fseek</code>, and <code>ftell</code> from the standard library.</li><br>
                        </ul><br>
                    </section>
                    
                    <section>
                        <h2>Error Handling</h2><br>
                        <p>It's essential to handle exceptions and errors during file operations, such as when a file doesn't exist or a write operation fails.</p><br>
                    </section>
                    
                    <section>
                        <h2>Security and Permissions</h2><br>
                        <p>When working with files, ensure appropriate permissions are set for file operations.</p><br>
                        <p>Take care to handle sensitive data securely.</p><br>
                    </section>
                    
                    <section>
                        <p>File handling is a crucial aspect of programming that allows for data persistence, storage, and manipulation across different applications and platforms.</p><br>
                    </section>
                    
                </div>

            </div>

            <!-- New section for notes -->
            <div class="notes-section">
                <h2>Save Your Useful Notes</h2>
                <div class="input-container">
                    <textarea id="notes" rows="10" cols="50" placeholder="Enter your notes here..."></textarea>
                </div>
                <br>
                <button id="save-btn">Save</button>
            </div>
            <a href="main.html" class="home-button">&#8962;</a>
        </div>
x
    </main>

    <footer>

        <div class="footer-wrapper" >
            <div class="footer-link-heading">Contact Us
            <div class="gfg-info">


                <a href="https://mail.google.com/mail/?view=cm&to=majorproject2024cse@gmail.com&su=&body=&bcc=" class="gfg-info-elems"><span class="material-symbols-outlined" style="color: var(--gfg-green); padding: 5px;">mail</span>majorproject2024cse@gmail.com</a>
                
            </div>
            
            <a href="about.html"><div class="footer-link-heading">About Us</div></a>
        </div>


            <div class="footer-strip">

            </div>

        </div>

    </footer>

</body>

</html>
